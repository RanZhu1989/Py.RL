# 第7章 N步自举法  
>MC方法和TD(0)方法（Sarsa、Q-learning）都可以用N步自举法框架来概括，对于MC方法来说是一种无穷步自举法，以至于不需要预测，因为从采样轨迹就可以计算回报，只需要采样后前推即可；而对于TD(0)方法则是一种单步自举法，采集执行一步操作后的下一个状态及收获，然后预测下一个状态的价值来替代后面回报的期望，这样看来MC和TD(0)似乎就是N步自举法的两种极端。与TD(0)单步方法相比，需要更多的内存来储存经验(S,A,R,S')，这个问题在后面**资格迹**方法中得到缓解。  

## 1.N步时序差分预测-估计`$V_{\pi}$`  
### 从TD(0)到N步时序差分
我们在使用增量式方法更新策略`$\pi$`下的某个状态的值函数时，每次加入的该状态回报的值可以是采样\仿真得来的（如MC），也可以是包含估计的(TD方法)，总之对于一个无模型的问题来说后续回报G只能算是一种估计（模型完备的问题直接DP求解），因此：  
```math
V_{\pi}^{new}(S_t)\leftarrow V_{\pi}^{old}(S_t)+\alpha[\hat G(S_t)- V_{\pi}^{old}(S_t)]\\
\hat G(S_t)=R(S_t,A_t)+\mathbb{E}[G(S_t)]
```
其中`$E[G(S_t)]$`对于MC方法来说是本次采样值，而对于TD(0)方法则是下个状态价值的估计，增加采样步数可以对状态信息掌握的更全面，由于折扣的存在，在最后一步截断采样然后用估计代替后续的回报似乎能够结合MC与TD(0)的特点，我们用下标t:t+n代表从t时刻开始采样，然后再t+n时刻截断，当n=1时就是TD(0)，当n=2时：  

```math
G_{t:t+2}=R_{t+1}+\gamma R_{t+2}+ \gamma ^2V_{t+1}(S_{t+2})

```
以及，对于一般的n有：  

```math
G_{t:t+2}=R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{n-1} R_{t+n}+ \gamma ^ n V_{t+n-1}(S_{t+n})
```
将上面的n步差分回报预测带入增量式状态价值函数估计中：  

```math
V_{t+n}(S_t)\leftarrow V_{t+n-1}(S_t)+\alpha[G_{t:t+n}-V_{t+n-1}(S_t)]\\
=V_{t+n-1}(S_t)+\alpha[R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{n-1} R_{t+n}+ \gamma ^ n V_{t+n-1}(S_{t+n})-V_{t+n-1}(S_t)]
```
这就是**N步时序差分预测法**
>`$V_{t+n}(S_t)$`下标的`$t+n$`代表：**在`$t+n$`时刻去更新内存中`$V(S_t)$`的值**，实际上也只能是这个时刻去更新，因为需要在t+n时刻才能知道状态`$S_{t+n}$`是什么。

>`$V_{t+n-1}(S_{t+n})$`下标的`$t+n-1$`代表：**在`$t+n-1$`时刻内存中`$V(S_{t+1})$`的值**，这实际是查找内存中已有的t+n步时所达到状态的对应的价值。  

>`$V_{t+n-1}(S_{t})$`下标的`$t+n-1$`代表：在`$t+n-1$`时`$V(S_t)$`的值，**实际上，只有再次遇到`$S_t$`这个状态后再隔N个步长才会更新V值，在此期间V值实际不变 。** 对于n步时序差分法，t时刻的状态`$S_t$`的价值至少应在t+n时刻才能更新（当然也可以延后更新，只要内存够大），t+1时刻的状态`$S_{t+1}$`的价值则要后推一个时刻，即每遇到一个状态，至少延迟n步才能更新该状态的价值。


![image](8DD0EEF74422407B8FE31586B3698243)
### N步时序差分预测算法  

```
n步时序差分算法（策略评估） 用于估计V_{pi}
输入：待评估策略Pi
参数: Alpha , gamma, n(时序差分步数)
初始化：V(S)
循环生成幕：{
    选择一个起点S_0 起点时刻为0时刻
    T=big M（一个很大的数，几乎不可能被达到）   #模拟终止态
    从起点时刻0时刻开始，对于每个时刻t遍历：{
        if t<T
            a=Pi(S_t)
            执行a,获取奖励R_t+1,转移到S_t+1
            if S_t+1=S_T              #如果下个状态是终止状态
                T=t+1               #记录下终止态时刻
        tau=t+1-n                   #tau是首个可以更新V值的时刻    
        if tau>=0                   #第一个被更新的状态从0时刻开始，如果没有可能更新的就跳过
            G= sigma 下标{i=tau+1} 上标{min (tau+n ,T)}，求和对象: r^{i-t-1} * R_i #求从tau后面一个时刻开始到截至状态（或提前结束幕）的折扣奖励之和
            if tau + n < T
                G =G +gamma^n * V(S_t+1)
            V(S_tau)=V(S_tau)+ Alpha *( G-V(S_tau) )
            else
            continue
    }
}
```
>为何对于从0开始的时刻t，第一个可被更新值函数的时刻`$\tau$`是`$t+1-n$`，而不是`$t-n$`？-因为在上面的代码中在t时刻执行了动作并转移到了下一个时刻（`$t+1$`时刻）的状态，所以下面的更新实际是以`$t+1$`时刻为基准的，故t先要+1再-n才是第一个可被更新V值的时刻

>当`$\tau+n<T$`，即时序差分步数超出了终止状态时，由于终止状态的价值`$V(S_T)$`就是0，所以这种情况下，在上面这段程序中只需要按采样求出`$S_T$`时刻前带折扣的奖励之和即可。
### N步回报的误差减少性质  
N步时序差分中用截断时刻的状态价值函数替代截断时刻后面的奖励求和，N步时序差分估计策略`$\pi$`下的状态价值的期望，与其真实价值的误差最多不超过更新前值函数最大误差的`$\gamma ^N$`倍，对于任意`$N\geq1$`，这被称为时N步回报的误差减少性质。（书中没证明）
```math
\max _{s}\left|\mathbb{E}_{\pi}\left[G_{t: t+n} \mid S_{t}=s\right]-v_{\pi}(s)\right| \leq \gamma^{n} \max _{s}\left|V_{t+n-1}(s)-v_{\pi}(s)\right|
```

## 2.N步Sarsa控制  
>使用N步时序差分方法可以预测状态的价值，回顾第六章内容中我们在使用TD(0)完成策略评估后，用`$\epsilon$`贪婪策略生成同轨策略，这就是TD(0)的Sarsa算法，在本节我们将尝试**用n步时序差分**来替代TD(0)的单步时序差分，称为n**步Sarsa**，把原先TD(0)下的Sarsa称为单步Sarsa或者Sarsa(0)。

在MC方法中，我们第一次碰到了强化学习算法进行**控制**的说法，到目前为止我们讨论的都是**表格型强化学习问题**，控制的思路是采用某个**固定的动作选取策略**（如贪婪策略）依据**Q表格**来选取动作，智能体通过与环境的交互（策略评估）来完善Q表格，再用动作选取策略产生输出动作。  
与MC相比基于n步时序差分的方法在开始的n步后就可以每步对前面经历的状态Q值进行更新，这样使得Q表格是**实时更新**（虽然不像MC那样最后一起更新，但是在过程中就可以更新一些值函数，对于在线性能很有好处），因此在动作选取策略下其对应状态输出动作也会更新，形成GPI的收敛；与TD(0)相比，如果在截断处遇到奖励的波动，价值的波动可以**传播**到截断处前n步每步的值函数上，使学习更快。

### N步Sarsa的Q值更新  
从回溯图很容易把单步Sarsa(Sarsa(0))推广到n步Sarsa甚至是n步期望Sarsa，图中虚线空心箭头代表在此处截断，用Q值去估计后续轨迹总收益。
![image](00CC4731B90E4AC78D96C3F8E5004C11)  
Sarsa(0)的Q值更新方式：  
```math
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha [R(S_t,A_t)+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
```
现在用n步时序差分策略评估替代原先Sarsa(0)中的单步时序差分策略评估部分：  
```math
if \ n\geq0 \ and\  0\leq t<T-n\\
Q_{t+n}(S_t,A_t)\leftarrow Q_{t+n-1}(S_t,A_t)\\
+\alpha [R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{n-1} R_{t+n}+ \gamma ^ n Q_{t+n-1}(S_{t+n},\pi(S_{t+n}))-Q_{t+n-1}(S_t,\pi(S_t))]\\
=Q_{t+n-1}(S_t,A_t)+\alpha [G_{t:t+n}^{\pi}-Q_{t+n-1}(S_t,A_t)]\\
else \\
Q_{t+n}(S_t,A_t)\leftarrow Q_{t+n-1}(S_t,A_t)\\+\alpha [R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{k} R_{T}-Q_{t+n-1}(S_t,A_t)]

```
>与N步时序差分预测算法类似，以t时刻的状态动作对为起始点，在经过n步后变为可更新其Q值，其中Q值的下标提示在t+n时刻可被更新，而t+n-1下标代表当前内存中的Q值。当n步轨迹生成中遇到终止状态而提前终止，其终止态Q值为0，认为在该n步轨迹上是一小段MC更新，因此只需要计算已收取的回报部分的和即可。

### N步Sarsa算法的伪代码  
以3步Sarsa为例，启动时前3步动作是依据初始Q表格生成动作，在第3步时开始从初始状态动作对开始更新Q表格。  
![image](8F99343518D3462884B5F8AFC024E6FD)
```
n步Sarsa控制
参数：Alpha ,gamma , epslion ,步数n
初始化 Q表
循环产生一幕:{
    选取一个初始状态S_0
    产生初始动作A_0=pi(S_0) pi是epslion-贪婪策略
    T=big M
    fot t=0,1,2,...
        执行 A_t, 转移状态到S_t+1, 收取奖励R_t+1
        if 转移后状态S_t+1是终止态S_T
            T=t+1
        else
            预测下一时刻的状态的动作并储存：SAVE A_t+1=pi(S_t+1)
        tau=t+1-n #当前时刻可更新Q的第一个时刻
        if tau>=0                   #第一个被更新的状态从0时刻开始，如果没有可能更新的就跳过
            G= sigma 下标{i=tau+1} 上标{min (tau+n ,T)}，求和对象: r^{i-t-1} * R_i   #计算轨迹上收取的奖励之和的部分，从最早可更新Q值的时刻加到截断位置或者终止态
            if tau + n < T
                G = G + gamma^n Q( S_t+n ,pi(S_t+n) )
            Q(S_tau, A_tau)=Q(S_tau, A_tau)+ Alpha *( G-Q(S_tau, A_tau) )
        else
            continue
            
                
}
```
### n步期望Sarsa  
从前面的回溯图可以看出，n步期望Sarsa与原先单步期望Sarsa相比在采样轨迹的长度上有所伸展，在处理截断处动作的Q值期望时是一致的:  

```math
Q_{t+n}(S_t,A_t)\leftarrow Q_{t+n-1}(S_t,A_t)\\+\alpha [R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{n-1} R_{t+n}\\ + \gamma ^{n} \sum _{a}\pi(a|S_{t+n})Q_{t+n-1}(S_{t+n},a)-Q_{t+n-1}(S_t,A_t)]
```
实际上，对Q求期望就是V，期望项被称为是状态`$S_{t+n}$`的**期望近似价值**，我们用`$\bar V_{t+n+1}(S_{t+n})$`来表示这个期望，如果状态是终止态，那么它的期望近似价值是0。  

```math
\bar V_{t+n+1}(S_{t+n})=\sum _{a}\pi(a|S_{t+n})Q_{t+n-1}(S_{t+n},a)
```
一般的，对于某个状态s，该状态的**期望近似价值**就是:  

```math
\bar V(s)=\sum _{a}\pi(a|s)Q(s,a)
```
期望近似价值在后续研究中会再次涉及。
## 3.基于重要度采样的n步离轨策略学习  
>MC方法可以看作是无穷步时序差分方法，在MC的离轨学习策略中行为策略`$b$`负责产生交互，当一条轨迹结束后，通过交互所得数据更新为目标策略`$\pi$`下的Q表`$Q_{\pi}$`。在n步时序差分方法中，我们将轨迹生成时某一个动作点截断，然后开始更新目标策略下的Q值，与MC不同的是，采用n步时序差分后在初始时刻n步后就可以在线进行Q值更新了，而不必等到每幕结束。  
### n步离轨策略学习下的重要度采样权重  
在离轨策略学习中，由于行为策略`$b$`需要满足对于目标策略`$\pi$`的覆盖假设。在MC中，我们依据`$b$`与`$\pi$`同时生成某一条完整轨迹的概率比来定义行为策略价值与目标策略价值的转换系数`$\rho_{t:T-1}$`（T为终止状态）：  

```math
\rho_{t:T-1}\doteq \prod_{k}^{T-1}\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}
```
同样，用n步时序差分法，行为策略与目标策略生成的相同特定的截断序列轨迹概率之比也可以定义为截断轨迹上`$Q_{\pi}$`与`$Q_{b}$`的转换系数`$\rho_{t:T-1}$`（n为时序差分步数，t为待更新Q值所在时刻）：  
```math
\rho_{t+1:t+n}\doteq \prod_{k=t+1}^{min(T,t+n)}\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}
```  
> 为何`$\rho$`的下标是从t+1到t+n？ 因为在控制问题是从动作-状态二元组出发，初始时刻(t时刻)的初始动作`$A_t$`已被确定，真正需要决策的是t+1时刻的动作，而尽管t+n时刻的`$Q(S_{t+n},A_{t+n})$`是一个预测值，但它的动作`$A_{t+n}$`依然是通过行为策略`$Q_{b}$`选取的（注意与Q学习的区别，Q学习在Q值更新中隐含了贪婪的目标策略，从而不需要重要度采样的权重设置）。

>如果截断序列轨迹中出现了目标策略中不应涉及的状态动作，那么`$\pi(A_{k}|S_{k})=0$`，**该段序列不应作为更新依据**，在MC中我们选择弃用该轨迹重新生成，而在n步差分的控制算法中，对于待更新状态我们可以选择不更新即可。  
### 基于重要度采样的离轨`$Q_{\pi}$`更新方法  
在n步Sarsa控制策略的基础上，行为策略与目标策略分开，在Q值更新时预测动作由行为策略产生，用行为策略的Q更新通过重要度采样权重来间接更新目标策略的Q，形成基于重要度采样的`$Q_{\pi}$`更新，如下表示：  
```math
Q_{t+n}(S_t,A_t)\leftarrow Q_{t+n-1}(S_t,A_t)\\
+\alpha \rho_{t+1:t+n} [R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{n-1} R_{t+n}+ \gamma ^ n Q_{t+n-1}(S_{t+n},b(S_{t+n}))-Q_{t+n-1}(S_t,b(S_t))]\\
```
### 离轨策略下n步Sarsa控制算法  
与先前同轨策略的算法相比，首先离轨策略下的Sarsa控制所用的动作预测是基于行为策略b，目标策略隐含在重要度采样权重中，因此算法最终形成的是`$Q_{\pi}$`
```
算法：离轨策略下的n步Sarsa 目标生成pi*
参数：Alpha, n
初始化： Q表 目标策略pi 行为策略b，b满足覆盖假设
循环生成一幕：{
    选取一个初始状态S_0
    产生初始动作A_0=b(S_0) b是一类软策略
    T=big M
    for t=0,1,2,...
        执行 A_t, 转移状态到S_t+1, 收取奖励R_t+1
        if 转移后状态S_t+1是终止态S_T
            T=t+1
        else
            预测下一时刻的状态的动作并储存：SAVE A_t+1=b(S_t+1)
        tau=t+1-n #当前时刻可更新Q的第一个时刻
        if tau>=0                   #第一个被更新的状态从0时刻开始，如果没有可能更新的就跳过
            G= sigma 下标{i=tau+1} 上标{min (tau+n ,T)}，求和对象: r^{i-t-1} * R_i   #计算轨迹上收取的奖励之和的部分，从最早可更新Q值的时刻加到截断位置或者终止态
            if tau + n < T
                G = G + gamma^n Q( S_t+n ,b(S_t+n) )
            rho= k从tau+1时刻开始至tau+n或T时刻结束，pi(A_k|S_k)/b(A_k|S_K) 连乘  
            Q(S_tau, A_tau)=Q(S_tau, A_tau)+ Alpha *rho *( G-Q(S_tau, A_tau) )
        else
            continue
}
```

### 基于重要度采样的n步期望Sarsa的`$Q_{\pi}$`更新方法
在基于重要度采样的离轨`$Q_{\pi}$`更新方法的基础上，将预测的Q变为期望近似价值，通过重要度采样权重转换到目标策略下的Q值：  

```math
Q_{t+n}(S_t,A_t)\leftarrow Q_{t+n-1}(S_t,A_t)\\
+\alpha \rho_{t+1:t+n-1} [R_{t+1}+\gamma R_{t+2}+ \gamma ^2 R_{t+3}+... \gamma ^{n-1} R_{t+n}+ \gamma ^ n \sum _{a}\pi(a|S_{t+n})Q_{t+n-1}(S_{t+n},a))-Q_{t+n-1}(S_t,b(S_t))]\\
```
>注意：`$\rho$`的下标终止是到t+n-1时刻，这是因为在t+n时刻我们实际上求的是Q的**期望近似价值**，这与前面的采样和预测得到的价值都不同，期望形式实际对应的是多条轨迹而不是某一特定的轨迹，所以重要度采样权重对其无意义，t+n时刻没有可用于生成轨迹的预测和采样产生。

## 4. n步树回溯-不使用重要度采样的离轨策略学习
> 在期望Sarsa、n步期望Sarsa中，我们在每个TD截断处不再以某个特定动作的Q函数来替代后续累计回报，而是按目标策略概率展开求期望，即该状态的期望近似值`$\bar V(s)=\sum _{a}\pi(a|s)Q(s,a)$`。在上节的末尾中尝试将n步期望Sarsa通过加入重要度采样权重变成离轨策略，我们注意到由于n步期望Sarsa中TD截断处变成了目标策略`$\pi$`下的期望形式，所以不需要再乘以重要度采样权重。受此启发，我们是否可以在其他状态节点也采用类似的期望形式来去掉重要度采样权重？  

### 从n步期望Sarsa到n步树回溯
**n步树回溯**(**Tree-backup**)是一种将目标策略`$\pi$`动作选择概率要素融入Q值计算的方法，先前的方法都是**由行为策略b生成一条特定的轨迹**，甚至在截断处都需要用策略b来预测后续轨迹，而树回溯方法相比确定的采样轨迹**增加了对其他可能发生动作的期望**，这种期望与目标策略`$\pi$`有关。   

下图中间和右边都是n步树回溯方法的回溯图，按照前面n步时序差分Sarsa伪代码中的设定，假设`$Q(S_{\tau},A_{\tau})$`是在`$t+1$`时刻应被更新的Q值，在`$\tau +1$`时刻通过策略b采取了动作`$A_{\tau +1}$`，但在状态`$S_{\tau +1 }$`下有许多其他动作可供选择，在每个后续状态都是这样的情况，直到截断时刻`$t+1$`时刻，此时刻下只收获延迟奖励而不再选择动作，与n步期望类似地将采用期望近似值来替代后续回报。

![image](88DA93C973F14B73BDD908EF8A91F372) 
### n步树回溯下的回报计算
以t时刻的状态动作对`$(S_t,A_t)$`为例，自t时刻开始的**状态动作对**`$(S_t,A_t)$`到t+n时刻**状态**`$S_{t+n}$`的**截断回报**`$G_{t:t+n}$`可以表示为`$R_{t+1}+\gamma \mathbb{E_{\pi}}G_{t+1}$`，即后面时刻奖励加上后时刻G值的期望。期望项按目标策略`$\pi$`的动作选择概率的权重展开，权重设置如下图所示。注意分类两类：红色箭头的确定性动作（主干）以及黑色虚线的可能动作（叶子），红色路线的G值可以由截断回报的剩余部分表示，即中间绿色部分：  

```math
G_{t:t+n}^{Tree}=R_{t+1}+ \gamma \sum _{a \neq A_{t+1}} \pi(a|S_{t+1})Q_{t+n-1}(S_{t+1},a)+ \gamma \pi (A_{t+1}|S_{t+1})G_{t+1:t+n}
```
>Q值中的下标t+n-1表示原先内存中的值

>为了与先前的截断回报区分，这里上标加入了Tree表示n步树回溯下的截断回报

![image](246EA44ED5FA449F9A640341D6FFCCCF)
类似的，不断从绿色部分的最上端画出红色部分，每次都可以用上式递归，直到倒数第二个状态执行最后一步动作转移到TD过程中最后一个状态时，这一单步回报与期望Sarsa中的计算方式相同。以图中为例：在`$S_t$`状态执行动作`$A_t$`后转移到`$S_{t+1}$`状态时，`$G_{t:t+1}=R_{t+1}+\gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1},a)$`。  
所以，在计算截断回报`$G_{t:t+n}$`时，可以从**最后一步回溯来递归式计算**从顶端`$(S_t,A_t)$`开始的回报，如上图中右边所示，先计算红色，再递归计算绿色，这样每次都以是计算上一个状态动作二元组出发至下一状态的“单步期望Sarsa”的更新目标。

### 基于n步树回溯的离轨控制算法  
与MC、TD(0)、n步TD方法一样，n步树回溯的Q值更新方法同样是用**从状态动作对出发至截断状态的G值**去替代**更新目标Q**:  

```math
Q_{t+n}(S_t,A_t)\leftarrow Q_{t+n-1}(S_t,A_t)+ \alpha (G_{t:t+n}^{Tree}-Q_{t+n-1}(S_t,A_t))
```
式中`$G_{t:t+n}^{Tree}$`与n步TD相比加入了与目标策略`$\pi$`相关的权重来估计回报，不再需要额外对TD误差进行再修正。  
其算法伪代码如下所示，主要框架和前面的类似，将学习目标换成n步树回溯下的：  

![image](40DBDC37C6AE4556B890EFB60386731D)
依然是t时刻先转移到`$S_{t+1}$`，然后再进行Q值估计，实际执行计算的时刻是t+1时刻。  
