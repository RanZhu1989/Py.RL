# 第3章 有限马尔可夫决策过程  
>与第2章的赌博机问题不同，MDP关心在某一状态下的值，包括状态的价值`$V(s)$`以及状态-动作二元组的价值`$q(s,a)$`.
## 1.MDP框架  
>智能体：学习和实施决策；  
环境：表现状态，产生收益;  
动作: 智能体对环境的作用  
注意：在MDP中，收益R是由环境**延迟**产生的，一个状态-动作-收益时间序列示例如下表示：  
> 
> ```math
> S_0,A_0,R_1,S_1,A_1,R_2,S_2 ...
> ```  
**几个重要的参数函数**：  
**1.四参数函数**`$p(s',r|s,a)$` **(概率）**   
完整描述了MDP的动态特性，也就是：在状态s下执行动作a后，状态转移到s'并收获延迟收益r的概率，可以理解为**问题的完整模型**  

```math
p(s',r|s,a)\doteq Pr(S_{t+1}=s',,R_{t+1}=r|S_t=s,A_t=a)
```  
从某一状态s出发，执行某一动作a后，所有的可能到达的s'与r组合的情况概率之和为1

```math
\sum_{s',r}p(s',r|s,a)=1
```  
**2.三参数函数**`$p(s'|s,a)$` **（概率）**   
四参数函数将收益情况整合后的一种形式，可以理解为**状态转移函数**或**系统的动力学模型**  

```math
p(s'|s,a)\doteq \sum_{r}p(s',r|s,a)
```  
**3.二元组函数**`$r(s,a)$`**(收益期望)**  
二元组函数定义了状态s下执行动作a后，环境返回的**收益的期望**  

```math
r(s,a)\doteq \sum_{s',r}rp(s',r|s,a) 
```  
后面我们将对其进行升级为一种更有远见，带折扣的期望，也就是`$q(s,a)$`  
**4.三元组函数**`$r(s,a,s')$`**(收益期望)**  
在上面二元组函数上针对转移后状态s'的情况求收益期望，可以从下面的定义里面包含了一个条件概率  
```math
r(s,a,s')\doteq \sum_{s',r}r \frac{p(s',r|s,a)}{p(s'|s,a)}
```  
## 2. 回报与分幕  
**回报：**  
与收益不同的是，回报一种长期收益，t时刻的回报用符号`$G_t$`表示，一种很自然的想法是把后面所有时刻的收益相加，当然，是从延迟收益`$R_{t+1}$`开始。  

```math
G_t=R_{t+1}+R_{t+2}+R_{t+3}+...
```  
对于持续型任务，按上述方法计算的回报会趋于无穷，为了使这个回报有界，我们引入了**折扣因子**`$\gamma$`，`$0<\gamma<1$`折扣后的回报计算公式如下：  
```math
G_t=R_{t+1}+ \gamma R_{t+2}+\gamma^2R_{t+3}+...
=R_{t+1} + \gamma G_{t+1}
```  
由无穷级数可以知道`$G_t=\sum_{k=0}^{\infty}r^{k}=\frac{1}{1-\gamma}$`  
**分幕：**  
分幕又称为episode，一个无限的MDP可以分为一些子序列过程，每个MDP子序列可以称为一幕，典型的场景如：一局棋局或牌局、一局电脑游戏、一局走迷宫等。  
每一幕总会开始于一个指定状态，结束于一个**特殊状态**，**每一幕结束状态与它下一幕的开始状态完全无关**。  
分幕结束的特殊状态可以人为设计成100%转移到自身，且收益为0。
## 3.策略与价值函数  
>几乎所有的强化学习算法都涉及价值函数的计算。**价值函数**是用来评估一个状态，或者是一个状态-动作二元组**回报的期望值**，这个期望值描述了“**在未来有多好**”  

**1.策略下的价值函数**  
策略`$\pi(s|a)$`代表了当前状态s到执行动作a**概率**之间的一种**映射**，强化学习的学习最终体现在策略的变化上。  
在某种策略`$\pi$`下，状态s的价值函数为`$v_{\pi}(s)$`，代表了从状态s开始的回报期望值，也称为策略 `$\pi$`的状态价值函数。

```math
v_{\pi}(s)=E_{\pi}[G_t|S_t=s]
```  
类似的，状态-动作二元组的在策略`$\pi$`下的动作-状态价值函数为：  

```math
q_{\pi}(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]
```  
状态价值与状态-动作二元组价值关系可描述为：  

```math
v_{\pi}(s)=\sum_{a} \pi(s,a)q_{\pi}(s,a)
```

>价值函数是可以通过蒙特卡洛实验方法来求得的，只要试验次数够多，计算平均值就可以趋近于真实的价值函数值，然而当问题复杂化后实验法就不太可行了。我们届时会采用时序差分（TD）

**2.价值函数的贝尔曼方程**  
还记得赌博机问题中的估计值迭代吗？在强化学习和动态规划中，价值函数同样存在递归迭代关系，在推这个递归关系时，我们需要借助**回溯图**，起始状态为S，在策略`$\pi$`下可能的动作假设为a1,a2,a3...，执行这些动作又会进入如s'11,s'12等下一状态。  
```
graph TD
S-->|pi|a1((a1))
S-->|pi|a2((a2))
S-->|pi|a3((a3))
a1-->|四参数函数p,r|s'11
a1-->s'12
a2-->s'21
a2-->s'22
a3-->s'31
a3-->s'32
```  
下面开始推导：  
```math

V_{\pi}(s)=E_{\pi}[G_t|S_t=s]\\
=E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_{t+1}=s']\\
=\sum_{a}\pi(s|a)\sum_{s',r}p(s',r|s,a)(r+\gamma E_{\pi}[G_{t+1}|S_{t+1}=s'])\\
=\sum_{a}\pi(s|a)\sum_{s',r}p(s',r|s,a)(r+\gamma V_{\pi}(s'))

```  
以上就是**状态价值函数的贝尔曼方程**，说明**先前状态的价值**等于**下一状态的期望收益**加上**带折扣的下一状态的价值**。  
同样可以推出状态-动作二元组的贝尔曼方程：
```
graph TD
a((a))-->|二元组期望r,三参数概率函数p|s'1
a-->s'2
s'1-->|pi| a'1((a'1))
s'1--> a'2((a'2))
```  
下面用之前定义过的二元组期望，以及三参数概率函数来计算：
```math

q_{\pi}(s,a)=r(s,a)+\gamma E_{\pi}[V_{\pi}(s')]\\
=r(s.a)+\gamma \sum_{s'}p(s'|s,a)V_{\pi}(s')\\
=r(s.a)+\gamma \sum_{s'}p(s'|s,a)\sum_{a'}\pi(s',a')q_{\pi}(s',a')
```
以上是**状态-动作二元组价值函数的贝尔曼方程**  
## 4.最优策略与最优价值函数
>强化学习的目标是寻找一个合适的策略，在此策略决策指导下最大化长期收益。  

**1.最优策略与最优价值函数**  
 在策略`$\pi$`的决策指导下，对于所有的状态`$S\in \mathcal{S}$`,都有`$V_{\pi}(S)\geqslant V_{\pi'}(S)$`，称该策略`$\pi$`为**最优策略**，记作`$\pi*$`，这里`$\pi’$`指其他策略。注意这里存在等号，指**该策略下所有状态的价值函数都不小于其他策略下的对应的价值函数**，由于这个关系只针对状态的价值，所以最优策略可能不止一种（可能存在风格迥异截然不同的几种策略，但它们最终都使价值函数值是最好的）。即使这些策略看起来完全不同，但它们对于每个状态S的价值函数值都是相同的，称为**最优状态价值函数**`$v*$`  

```math
v_{*}(s) \doteq \max\limits_{\pi} v_{\pi}(s)
```  
如果一个策略`$\pi$`是最优策略，类似的对于状态-动作二元组的价值，**最优状态-动作价值函数**`$q_{*}(s,a)$`可以定义为：  

```math
q_{*}(s,a) \doteq \max\limits_{\pi} q_{\pi}(s,a)
```  
>注意，在最优状态价值函数中，从状态s开始，最优策略就开始指导进行下一步的动作；而在最优状态-动作价值函数中，由于已指定了状态动作二元组(s,a)，所以最优策略实际是从下一个状态即状态s'才开始进行决策指导。  

最优动作a*形成的期望回报`$r(s,a^*)$`，与下一个状态s'的“带折扣的长期回报”（用`$\gamma v_{\pi}(s')$`表示) 构成了`$q_{\pi}(s,a^*)$`
```
graph TD
S-->|已指定|a1((a1))
S-->|X不执行X|a2((a2))
a1-->|收获奖励+折扣长期回报|s'11
a1-->|收获奖励+折扣长期回报|s'12
a2-->s'21
a2-->s'22
s'11-->|由策略决定|a'111((a'111))
s'11-->|由策略决定|a'112((a'112))
```

```math
q_{*}(s,a)=r(s,a)+E[\gamma v_{*}(s')]
```  
>注意：这里的期望不再与策略有关，而是由环境驱动，因为状态动作已指定。后面一项表示的是“被环境驱动到下一状态的最优状态价值函数”的期望值  

**2.最优价值函数的贝尔曼方程**  
最优价值函数也是价值函数，所以它必定满足价值函数的贝尔曼方程。  
最优状态价值函数可以由`$v_{\pi}(s)=\sum_{a} \pi(s,a)q_{\pi}(s,a)$`将策略定为最优策略`$\pi*$`开始推导:

```math
v_{*}(s)=\sum_{a} \pi_*(s,a)q_{\pi*}(s,a)\\
=E_{\pi*}[q_{\pi*}(s,a)]\\
```  
我们要找到使`$v(s)$`是最大的策略`$\pi*$`  
```math
v_{*}(s)=\max\limits_{\pi}E_{\pi}[q_{\pi}(s,a)]
```  
>一个最优状态价值函数`$v_*(s)$`暗示了：在状态s时先随意选择一个动作a，只要关于该状态-动作二元组的价值函数（在最优策略下，那怕我们不知道是具体那种策略）是最大的，那么该动作a就是符合最优策略的最优动作a*。选择该动作即实现了从状态s开始的最优演化。 

只要找到本状态s下最优动作a*，后面的值函数用最优策略下的变量表示，如此一步步迭代，就可以发现完整的最优动作序列，按这个思路：
```math
v_{*}(s)=\max\limits_{\pi}E_{\pi}[q_{\pi}(s,a)]\\
=\max\limits_{\pi}E_{\pi}[r(s,a)+ \gamma V_{\pi}(s')]\\
=\max\limits_{a}E[r(s,a)+\gamma V_{\pi*}(s')]\\
=\max\limits_{a} \sum_{s',r}p(s',r|s,a)(r+\gamma V_{\pi*}(s'))\\

```    
>把求最优完整策略分布转化为求当前状态下最优动作后，原先基于策略的期望由于最优动作的确定转变为给定参数下环境驱动的期望。由于最优动作a*形成的期望回报`$r(s,a^*)$`，与下一个状态s'的“带折扣的长期回报”（用`$\gamma v_{\pi}(s')$`表示) 构成了`$q_{\pi*}(s,a)$`，所以
`$v_{*}(s)=\max\limits_{a}q_{\pi*}(s,a)$`

类似的，状态-动作最优价值函数的贝尔曼方程由`$q_{\pi}(s,a)=r(s,a)+ \gamma \sum_{s'}p(s'|s,a)v_{\pi}(s')$`开始推导：  

```math
q_{*}(s,a)=\max \limits_{\pi}[r(s,a)+ \gamma \sum_{s'}p(s'|s,a)v_{\pi}(s')]
```  
第一项是由环境驱动的确定项所以不受策略影响，把二元组期望函数展开表示即可，第二项求最大的`$v_{\pi}(s')$`可以用上面的结论`$v_{*}(s)=\max\limits_{a}q_{\pi*}(s,a)$`带入后整理：

```math
q_{*}(s,a)=\sum_{r',s}p(s',r|s,a)(r+\gamma \max\limits_{a’}q_{\pi*}(s’,a’))
```
>可以发现状态最优价值函数迭代关键是确定本状态下下一步的最优动作，而状态-动作最优价值函数的迭代需要经过一次环境的驱动，下一个状态还无法确定，所以总体还是一个期望形式。

