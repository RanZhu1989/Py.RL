# 第八章 表格型规划与学习  
>从动态规划到学习算法，表格型问题都是基于状态或状态动作对的价值来生成最优动作序列。价值都是通过回溯来计算，动态规划与学习算法的不同点在于：动态规划具备环境的模型，而学习算法不具备环境模型，精确的环境模型甚至可以不与环境交互就能得到经验，而学习算法必须与环境交互来获取经验。本章实际是讨论的“有模型”的强化学习基本框架，尝试将学习与规划统一在一个框架内。  

### 1.模型与规划  
### 模型
**模型**指可以预测环境对智能体动作反应的黑盒，输入当前给定**状态**和**动作**，模型输出**奖励**和**下一状态**。模型有两种：**分布模型**和**样本模型**，分布模型包含了输入到所有可能输出的概率分布，例如先前的四元组概率函数`$p(s',r|s,a)$`；样本模型则直接按照概率直接输出一个具体动作，相当于进行一次试验。  
### 规划  
回顾第三章动态规划，无论是规划还是学习我们都需要依据价值来进行，强化学习主要关注**状态空间规划**，即一种“搜索”方法，价值是定义在状态上的，在状态空间中搜索基于状态转移的最优策略或最优转移序列，而状态的转移则是由动作所引发。另一种方法称为方案规划，例如进化算法和偏序规划，更新以整体方案为单位，不适合RL。  
### 状态空间规划基本思想  
1. 利用价值函数作为改善策略的关键依据；  
2. 基于对仿真经验的回溯来计算价值函数  

```
graph LR
模型-->仿真经验
仿真经验-->|回溯|价值函数
价值函数-->|改进|策略
```
>无模型的学习与状态空间规划非常相似，除了模型外，回溯过程可以是短期的也可以是长期的，这与无模型学习的n步时序差分相当类似。  
## 2.Dyna：规划、动作、学习集成框架  
### 经验的作用
智能体的动作可能会改变环境，因此初始模型需要进行不断改进。智能体与环境交互得获得实际采样的(s,s',a,r)，我们称为**经验**，而智能体通过模型得到的(s,s',a,r)被称为**仿真经验**。

```
graph LR
经验-->改进模型
经验-->估计价值及改善策略
改进模型-->模型学习
估计价值及改善策略-->直接强化学习
```
### 集成框架  
我们将学习与规划功能都融入智能体，智能体与环境交互获取的经验同时用于RL学习更新Q函数，以及更新模型，然后依据模型生成仿真经验再进行规划。按道理它们应该并行更新，但用模型的仿真经验进行规划时较耗时，因此可以看作是先用经验学习更新Q函数，然后再更新模型，用仿真经验对Q函数进一步验证和改进
![image](EB9AA52CAF5A4BDA95DB617BFC6EA3B8)

```
graph LR
智能体与环境交互获得的真实经验-->改善模型
改善模型-->|搜索控制|仿真经验
仿真经验-->|规划|策略评估与策略改进
智能体与环境交互获得的真实经验-->直接从经验中学习
直接从经验中学习-->策略评估与策略改进
```
>搜索控制：用模型生成经验时选择初始(S,A)  
### 表格型Dyna-Q算法  

```
表格型Dyna-Q算法
初始化： Q(S,A) Model(s,s',r,a)
循环{
    s=当前状态
    用epslion贪婪策略选取动作A，参考Q表
    执行动作A，获得奖励R，转移到下个状态S'
    Q(S,A)=Q(S,A)+alpha*(R+ gamma *argmax_a' Q(S',a)-Q(S,A))
    将(s,s',r,a)更新加入模型
    开始仿真规划，循环n次{
        随机选择一个模型中存在的状态s
        随机选择模型中s状态下执行过的动作a
        由(s,a)经模型得到(s',r)
        Q(S,A)=Q(S,A)+alpha*(R+ gamma *argmax_a' Q(S',a)-Q(S,A))
    }
    
}
```
>直接Q学习与规划都是增量式更新的，所以能够很好的兼容彼此。
### 模型加速训练的功能
在上面的算法中，如果循环次数n为0，那么就是纯粹的Q学习，智能体只有通过与环境交互才能获得学习机会，而模型的仿真经验可以在后台进行多次回溯更新，回溯轨迹都可以回到初始状态。
## 3.当模型错误时  
本节书中通过一种环境存在变化的迷宫示例，说明了Dyna-Q智能体也存在探索与利用矛盾的问题，智能体容易发现策略轨迹附近的环境变化，但难以察觉其他地方的变化（如书中的捷径），因此可能恰当的**启发式方法**能够更有效的完成任务。在书中提到了一种启发式方法：对一个状态-动作二元组进行跟踪，如果长时间这个二元组没有被访问到，我们更有理由相信这个二元组的Q值发生了变化，智能体被更多鼓励在这个状态采取相应的动作，但如何在这种试探和开发之间取得平衡，目前仍很难解决。  
## 4.优先遍历  
### 均匀采样遍历的问题
在前面的Dyna算法中的仿真模拟过程中，我们实际是在已知模型中随机选取已涉及的状态及可能的动作。其实早在第三章动态规划时，我们已发现某些状态的价值变化率很大，变化率大的状态影响了其最优策略，在图中我们看到值函数似乎在达到某个阈值后就会明显改变最优策略。
![image](27A188194BE34B338F5B63897585B113)
![image](AC5A9E45F7274D47820D64AC6FDE6A21)
这些图可能表述的还有些模糊，我们再考虑复杂的情况，在一些任务中，一些状态动作对客观上讲就是“低价值”的，比如迷宫中某些离障碍物近且远离出口的位置，我们的Dyna智能体每次在模拟仿真更新若涉及这些状态毫无意义，因此需要将计算资源倾斜到前面所述的一些“重要状态”上。  
### 反向聚焦  
我们在比较n步Sarsa与TD(0)方法时，以格子世界迷宫问题为例，最终目标状态附近的价值往往是最高的，价值从目标点向周围“耗散”。TD(0)方法每次更新逐步把从目标状态开始的附近一步的状态价值拔高，而n步Sarsa把从目标状态开始附近n步内的状态都拔高从而形成最优策略。很自然的想法就是：从目标状态出发回溯，着重更新这些似乎能形成最优路径的状态价值。但是目标状态是一种特殊状态，可能在开始时智能体并不能达到。
不论是从DP还是n步自举类方法，发现某一个状态动作的价值改变后，往先前回溯它们的先导动作状态价值也一定会改变，这种不断向前回溯更新先导价值的方法称为**反向聚焦**。

### 规划中的优先遍历 
反向聚焦的对象是值函数发生变化的状态或状态动作对，在学习过程中变化的状态或状态动作很多，更新那些无用价值只会浪费一次规划的机会，所以我们不可能跟踪所有的值函数变化，书中提出了利用价值改变大小作为权重的方法，对于采用Q学习的方法来说就是：  

```math
\theta=|R_{t+1} + \gamma \ \max_{a'}Q(S_{t+1},a')-Q(S_t,A_t)|
```
这是一种优势函数的绝对值，其中`$\theta$`是阈值，对于超出该阈值的状态动作对，按其大小设置权重加入一个队列中，先用模型仿真去检验\修正该价值（个人认为是用经验判断下这种剧烈变化的可能性，如果与模型仿真经验吻合那最好，如果不吻合说明可能是偶然事件）。接下来对所有先导状态-动作进行回溯，找出上一个超过阈值的先导状态-动作对，在回溯过程中`$R_{t+1}$`项是依据某种模型或预测得到S,A -> S',R。
下面是书中针对确定性环境下的优先级遍历算法：  
![image](D01F8D5145AB49BD88E59FB816F15B83)
与前面的Dyna-Q相比在采样后没有直接学习更新，而是寻找值函数变动超出阈值的状态动作对。算法中蓝色部分是用当前模型对(S,A,P)这个元组列表取出权重最大的元素进行模型核验更新，然后红色部分是对其所有可能的先导状态（单步）进行回溯更新，注意这里的奖励R，书中说是预测收益，可以理解为基于某种预测方法得到的（经验或完备模型），这里不太清楚。  
### 前向聚焦  
书中提出可以从一些经常能够访问到的状态-动作出发，向前估计哪些后续状态是容易到达的，重点更新这些状态的价值，就是**前向聚焦**。  
## 5.期望更新 vs.采样更新  
书的第I部分是表格型问题，依据以下原则对第I部分重点涉及的学习或规划方法进行总结：  

更新对象 | 策略下的\最优的 |回溯方式
---|---|--
V | 具体策略pi下的 |采样更新
Q | 最优值函数 |期望更新

更新对象 | 回溯方式 |方法 |对应章节
---|---|---|---
`$V_{\pi}$` | 采样更新 |TD(0)策略评估|6.1
`$V_{\pi}$` |  期望更新 |DP-V策略评估|4.1
`$V_{\pi*}$` | 采样更新 |-|-
`$V_{\pi*}$` |  期望更新 |DP-V价值迭代|4.4
`$Q_{\pi}$` | 采样更新 |Sarsa 中间过程|6.4 7.2
`$Q_{\pi}$` |  期望更新 |DP-Q策略评估|Q的贝尔曼方程
`$Q_{\pi*}$` | 采样更新 |Q-Learning|6.5
`$Q_{\pi*}$` |  期望更新 |DP-Q价值迭代|3.6

期望更新能够更好的更新价值，但是需要消耗更多的计算资源，并且需要依靠概率分布模型。而采样更新计算资源开销小，但是有采样误差的影响。  
以采样更新的Q学习与期望更新的Q贝尔曼方程作为比较：  
Q学习：
```math
Q(s,a)\leftarrow Q(s,a)+\alpha [R_{t+1}+\gamma \max\limits_{a'}Q(s',a')-Q(s,a)]
```
Q贝尔曼方程，但是注意四参数函数p是估计值：  
```math
Q(s,a)\leftarrow \sum_{s',r} \hat p(s',r|s,a)[r+ \gamma \max _{a'}Q(s',a')]
```
考虑一个更一般的存在随机因素的环境，从表达式看，采样更新的方法与期望更新的方法在性能上会拉开显著差距，这是因为期望更新的期望能够消除部分采样带来的偏差，这在前面也讨论过。但是并不能说我们更倾向选择期望更新，如果一个决策树非常复杂，那么期望更新需要计算所有的可能性，正如上面的`$\max _{a'}Q(s',a')$`项会消耗更多计算资源。  
书中用**分支因子**`$b$`来表示状态`$s$`所有可能达到的后继状态`$s'$`的数目，也即是对于状态`$s$`，期望更新方法消耗的算力大约是采样更新方法的`$b$`倍。书中这张图是假设所有后继状态`$s'$`都是等概率发生的，且开始时估计误差是1，可以看出对于分支因子较大的状态，该状态值函数更新误差在开始时（图中指的是相对于b的百分比，也就是与期望更新需要b次遍历相比）下降很大，而对于分支因子数目较小的状态，误差下降较慢，而对于期望更新，则是在遍历完b个分支后一次性更新使值函数偏差降到最低。  

![image](271A7624C43F4B629421787FFB570441)

## 6.轨迹采样  
前面为止介绍的方法都假定需要遍历整个状态-动作空间：从DP、MC、TD(0)、N步自举法再到先前Dyna、及前面提出的对遍历策略的改进全部都需要足够的交互，是一种**穷举采样方法**。在许多实际问题中虽然状态动作空间很大，但是相当一部分是“无价值”的，如果我们能够预测哪些状态-动作对于解决问题是关键的，重点去更新这部分的值函数，那么效率更高。  
一种方法是使用在线的同轨策略，遵循当前的同轨策略（策略会在线修正）的分布来分配更新值函数的算力，但是一个同轨策略的分布暗示了需要对完整的一幕轨迹采样才能准确获得，为了解决这个问题，可以采用与模型交互生产完整轨迹的方式来代替与环境交互，完全通过仿真经验来进行值函数更新称为**轨迹采样**。书中对于这个模型没有说明，个人认为是采用同轨策略收集的经验生成了实时更新的模型，轨迹采样在后台进行，就像Dyna一样。  
![image](8CB2DE17F62049CF843B940BBE530AD6)
书上这幅图说明了，采用轨迹采样在开始时学习\规划更新值函数的速度很快，但很快趋于平稳，因为开始那段时间同轨策略经常涉及的那些值函数已经更新完毕，只有采样到其他状态才能继续更新。换句话说，应用在那些本身状态空间很大，但根据经验所知实际状态空间可行域很小的情况会取得较好效果。  
## 7.实时动态规划(RTDP): 轨迹采样下的价值迭代  
实时动态规划(RTDP)结合了以下三种技术：  
**DP中的价值迭代**：  

```math
v_{k+1}(s)=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]=\max _{a} \mathbb{E}_{s^{\prime}, r \sim p}\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]
```  
**异步更新**：遇到一个状态就立即更新一个  
**轨迹采样**：依据某种策略（如贪婪策略）从当前状态与模型交互产生一条轨迹  

RTDP依据模型来访问感兴趣的小部分状态，价值迭代中的max操作已经融入到轨迹采样中的贪婪“行为策略”（这个策略在仿真中负责与模型交互采样）中了。   
RTDP在每一步用轨迹采样期望（平均）去取代上面价值迭代公式中的期望项。
由于算法用模型进行滚动预测，用预测轨迹中第一个贪婪动作来更新值函数。  
## 8.决策时规划  
DP以及Dyna框架的算法都是**以更新值函数为目标**，动作选择是依据策略在当前值函数中挑选动作（表格型-Qtable，连续型-近似函数），值函数更新任务可以在后台进行，利用的是采样或仿真两类手段得到的数据，前台任务是依据值函数挑选最优动作。书中提出的**决策时规划**指的是每当遇到一个新状态时，利用模型预测后续轨迹价值，然后从中选取动作，**直接输出动作**（映射策略）而不是更新值函数。  
## 9.启发式搜索  
## 10.预演算法  
利用MC控制的决策时规划算法，目标是提升预演策略的性能。  
## 11.蒙特卡洛树搜索 MCTS
 ![image](117F242CA64E4E639D1E6DDEEDEB3E67)
 MCTS实际是一种预演算法，同时平衡了强化学习中的探索与利用过程，上图中空心圆圈是状态，实心圆圈是动作，MCTS实现分为四步：  
 选择：从初始状态不断选择动作达到下一个状态，用书中的话说，这个状态是“有前景的”，直到一个没有继续往下走过的状态（没有执行过动作）。  
 扩展：用仿真方法进行快速rollout，直到终止  
 回溯：将仿真结果反向传播到上游各个节点，更新价值。