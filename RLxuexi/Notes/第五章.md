## 第5章 蒙特卡洛方法  
> MC方法是一类通过与环境（真实环境或模拟仿真）交互得到采样序列来估计值函数的方法。依据大数定律，在样本数量足够大时可以通过计算平均回报来解决强化学习问题，对于分幕式任务则可以在每一幕结束后进行价值估计和策略改进，也就是蒙特卡洛控制的价值迭代。。  
## 1.蒙特卡洛预测  -给定策略下`$v_{\pi}(s)$`估计
**1.首次访问型MC算法**  
>**问题背景**：在给定策略`$\pi$`下，考虑一个分幕式任务，Agent有多次达到了状态s，在某一幕中，每次状态s出现就称为对s的一次**访问**，在整个任务甚至在同一幕中，状态s有可能被多次访问到  

**首次访问**：第一次访问状态s被称为对状态s的首次访问  
用**首次访问的回报平均值**估计`$\pi_{s}$`就是**首次访问型MC**，用每次访问到状态s的所有回报平均值就是**每次访问型MC**。不论是每次访问型和首次访问型，当对s的访问次数趋于无穷时，只要策略保持不变，对于状态在策略`$\pi$`下的估计值都会收敛到`$\pi_{s}$`。  
**2.首次访问型MC预测算法伪代码**  



```
## 首次访问型MC预测算法 ##

输入：待评估策略
初始化 v(s)
初始化列表 L

循环N次{
  依据输入策略生成一幕序列：S0,A0,R1,S1,A1,R2,...S_{T-1},A_{T-1},R_{T}
  G=0
  for t in {T-1,T-2,...0}
    计算序列中每个节点的回报 G = gamma * G+ R_{t+1}
    G加入列表L #列表是每个状态的回报的表格，推到最后就是第一个状态S0的
    if St 在序列{S0,S1,...S_{T-1}}中出现过
      把列表中该状态的回报换成时间靠前的回报  #因为是从后往前考察
      continue
    end if
  end for
  }
  对于表格L中每种状态的回报，统计其平均值，就是各状态的价值函数估计
  
```
在策略`$\pi$`下从**待考察状态s开始**，以分幕的方式生成多条轨迹，对这些轨迹从最后一个状态开始通过`$G=\gamma G+R_{t+1}$`计算对应状态的平均回报，作为策略`$\pi$`下的**状态s的价值函数估计**。在首次访问型算法中，这条轨迹若经过了多次状态s，s状态的价值只**按最先出现的那个状态的回报**来估算。  
**3.MC方法与DP的比较**  
与DP不同的是，MC方法对于每个状态的值函数估计是独立的，每个状态价值函数之间没有明显关系（尊重试验事实），这不像DP是基于自举的推断方法。  
MC方法的回溯树就是一条确定的采样轨迹，而不是DP的树状形式。  
MC需要依赖一条完整的采样轨迹数据计算值函数，而DP依据上个状态值函数计算下一状态值函数。  这是也是MC方法的特色-**从实际环境采样和仿真模拟中学习**



```
graph LR
S0-->a1((a1))
a1-->|r1|S1
S1-->a2((a2))
a2-->|r2|S2
```
## 2.动作价值的MC估计法- 给定策略下`$q_{\pi}(s,a)$`估计
**动作价值函数的策略评估**：在DP问题中，由于模型完备，所以依靠状态的价值函数`$v(s)$`就可以用贝尔曼方程得到最优策略。而在一些无模型问题中，由于动力学方程或状态转移概率未知，所以需要依据状态-动作二元组的价值函数`$q(s,a)$`来实现表格型问题的策略生成。  
**MC估计法**：与上一小节中**状态价值函数**的估计方法类似，我们将**访问考察对象**改为**状态-动作二元组**，用同样的方法计算所有碰到的（或是第一次碰到的）状态-动作二元组价值函数的平均值作为其估计值。  
**MC方法对于确定性策略的困境**：对于概率型策略，次数足够大都能保证一些状态被访问到并更新，但对于确定性策略，一旦策略固定后只会转移到**特定的状态-动作二元组**上，也就是**某些状态-动作二元组在该策略下永远无法被访问到**，而策略提升需要收集尽可能多的值函数，而不是局限于某些值函数。因此，需要Agent**保持探索能力**。  
**MC的试探性改进**：  
有两种方法：试探性出发（**Exploring Start**)、随机策略（比如后面我们会用软贪婪策略）  
试探性出发就是随机将状态-动作空间中二元组作为起点，这样只要幕数足够，确定性策略中难涉及到的二元组就可能被全部访问到；随机策略就是将确定性策略变为“任何动作都有非零概率被选中”。  
## 3.基于蒙特卡洛的控制方法-MC GPI 
**基于理想条件的MC的广义策略迭代GPI**：   策略评估（E）后进行策略提升（I），策略评估使值函数接近真实值，策略提升选取最优动作，最终趋近于最优值函数和最优动作，这里借用原书中图片和知乎上图片。  
![image](6946DA0524384AD589CF4A3D227DD4A1)![image](https://pic4.zhimg.com/80/v2-4cbd1c0d42fe5c2be5abf6173e4f0765_720w.jpg)  
MC的GPI很容易实现，考虑下面的GPI序列，以`$\pi_{0}$`为初始序列，假设E代表**执行无数幕**（理想条件1），**假设满足试探性出发**（理想条件2），通过MC就可以遍历估计求得所有二元组值函数`$q(s,a)$`，通过贪婪方法将值函数映射到策略中：`$\pi(s)=arg \max_{a}q(s,a)$`
```math
\pi_{0}\xrightarrow[]{E}v_{\pi_{0}}\xrightarrow[]{I}\pi_{1}\xrightarrow[]{E}v_{\pi_{1}}\xrightarrow[]{I}\pi_{2}...\xrightarrow[]{E}{q_{*}}\xrightarrow[]{I}{\pi_{*}}
```  
策略改进(I)依据DP中的策略改进定理:  

```math
q_{\pi_{k}}(s,\pi_{k+1}(s))=q_{\pi_{k}}(s,arg\max_{a}q_{\pi_{k+1}}(s,a))\\=\max_{a}q_{\pi_{k}}(s,a)\geq q_{\pi_{k}}(s,\pi(s))
```  
k步的值函数用于策略提升形成k+1步的策略  
**去除“理想情况”条件**：  
理想条件1=策略评估中采用了无数幕来估计价值；理想条件2=MC采样轨迹起点随机：试探性出发 。试探性出发这个条件在生活中有一些情况是可以满足的，所以在本小节先研究去掉需要无数幕来估计价值这个条件，因为在DP中对于价值估计我们有“*不需要完全走完估计流程*”的**价值迭代**版本，因为贪婪策略只关心**q的相对大小**，迭代几次后就贪婪动作策略就可以生效了。  
在上述MC的GPI中，我们对于E中的分幕式估计法，在每一幕结束后进行策略评估，并立即进行策略提升（**注意这里提升策略是针对本幕涉及到的状态**）,这实际是一种”分幕式“**策略迭代**方式，这被称为**基于试探性出发的蒙特卡洛**（MC ES），下面是MC ES的伪代码实现：  

```
## MC ES（蒙特卡洛试探性出发策略迭代）用于估计最优策略pi* ##

初始化：策略pi(S),q(S,A),列表L
循环N次{

    随机选择一个起点(S0,A0)
    以该起点根据策略pi产生一幕序列 S0,A0,R1,S1,A1,R1...S_{T-1},A_{T-1},R_{T}
    G=0
    for t=T-1:0
    计算序列中每个节点的回报 G=gamma*G+R_{t+1}
    G加入列表L 
    if St 在序列{S0,S1,...S_{T-1}}中出现过
      把列表中该状态的回报换成时间靠前的回报  
      continue
    end if
    计算Q表格：将列表L元素值相加后除以出现次数得到各二元组平均价值
    依据Q表格，贪婪策略选取各状态S下“最优”动作A*
    把(S,A*)对应关系映射为新策略，即“最优”策略pi*
    
}
```  
## 4.没有试探性出发假设的蒙特卡洛控制
>在前面的研究中，我们假设保证能够**遍历整个状态-动作空间的元素**作为起始二元组(S0,A0)作为起点，来开始我们的序列采样。目前，试探性出发假设的解决方案就是要使Agent能够**持续不断地选择所有可能的动作**，有两种方法：on-policy和off-policy

**on-policy和off-policy**：  
同轨策略on-policy: 采样产生数据的策略，即**实际作用与环境的输出策略**（**behavior policy**)与用于待评估和改进的策略(**target policy**)是相同的  

离轨策略off-policy: **behavior policy**和**target policy**是不同的，实际操作的策略behavior policy“离开”了我们待评估和提升的策略  
>将behavior policy和target policy分开，一个很自然的想法就是对behavior policy进行改进，使得它能够遍历到各类状态收集数据。这样就不需要通过无限设置随机起点来强制遍历状态了（此时behavior policy相当于是一个确定策略）而target policy则是待生成的真正策略（应用时我们不需要训练了，即将target policy作为控制器）  

**去掉试探性出发假设的同轨策略的MC控制方法**：  
一种方法是用较“soft”的贪婪选择来代替贪婪选择，使得所有动作都有不为零的概率被选中，通过策略改进后的目标策略直接拿来应用于与环境交互使得遍历所有二元组成为可能，这也即是一种**同轨策略**。实际上，我们只需在上节中首先去除试探性出发的起点假设，然后将基于“完全”贪婪的确定性策略`$\pi(s)=arg \max_{a}q(s,a)$`改为soft贪婪策略即可：  

```
## 同轨策略下 首次访问型 MC控制算法 soft epsilon贪婪策略 ##

初始化：soft策略pi(S),q(S,A),列表L
循环N次{

    依据策略pi产生一幕序列 S0,A0,R1,S1,A1,R1...S_{T-1},A_{T-1},R_{T}
    G=0
    for t=T-1:0
    计算序列中每个节点的回报 G=gamma*G+R_{t+1}
    G加入列表L 
    if St 在序列{S0,S1,...S_{T-1}}中出现过
      把列表中该状态的回报换成时间靠前的回报  
      continue
    end if
    计算Q表格：将列表L元素值相加后除以出现次数得到各二元组平均价值
    依据Q表格，贪婪选取各状态S下“最优”动作A*
    
    将A*映射到soft策略pi：对于状态S下，大概率选取A*，小概率随机选取其他动作
```  
**soft`$\epsilon$`-贪婪策略的策略提升证明**  
我们需要同策略提升定理来证明，soft `$\epsilon$`-贪婪策略至少不会比原策略差。假设`$\pi'$`是一个新形成的soft`$\epsilon$`-贪婪策略，参考下面回溯图开始证明：
```
graph TD
S-->|新策略pi'|a1((a1))
S-->|新策略pi'|a2((a2))
S-->|新策略pi'|a3((a3))

```  

```math
q_{\pi}(s,\pi'(s))=\sum_{a}\pi'(a|s)q_{\pi}(s,a)
```  
由于`$\epsilon$`概率选择非贪婪动作，`$1-\epsilon$`概率选择贪婪动作，`$|A(s)|$`表示状态s下可执行动作的动作个数，所以上式中把软策略按概率展开：  

```math
q_{\pi}(s,\pi'(s))=\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi}(s,a)+(1-\epsilon)\max_{a}q_{\pi}(s,a)
```
由于下面一个式子成立，说明求和符号里面是权值，整个一项是加权平均：

```math
\sum_{a}\frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon}\\
=\frac{1}{1-\epsilon}\sum_{a}(\pi(a|s) - \frac{\epsilon}{|A(s)|})\\
=\frac{1}{1-\epsilon}(1-\epsilon)=1
```  
那么上式乘以`$q_{\pi}(s,a)$`就是对于`$q_{\pi}(s,a)$`的加权平均，这个加权平均肯定是小于其中最大的`$q_{\pi}(s,a)$`的，所以有：  

```math
q_{\pi}(s,\pi'(s))=\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi}(s,a)+(1-\epsilon)\max_{a}q_{\pi}(s,a)\\
\geq\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi}(s,a)+(1-\epsilon)\sum_{a}\frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon}q_{\pi}(s,a)\\
=\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi}(s,a)+\sum_{a}\pi(a|s)q_{\pi}(s,a)-\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi}(s,a)\\
=v_{\pi}(s)
```
**soft`$\epsilon$`-贪婪策略提升能收敛到最优策略吗？**   
当上述策略提升表达式中取等号，即策略无法提升时，有：  

```math
v_{\pi}(s)=\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi}(s,a)+(1-\epsilon)\max_{a}q_{\pi}(s,a)\\
=\sum_{a}\frac{\epsilon}{|A(s)|} \sum_{s',r}p(s',r|s,a)[r+\gamma v(s')]  
+(1-\epsilon)\max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v(s')]  

```
下面我们根据第3章中推导贪婪策略下最优值函数的贝尔曼公式的过程步骤中某一步：  

```math
v_{*}(s)=E[r(s,a)+ \gamma V_{\pi*}(s')]
```  
注意我们现在采用了soft策略，为了简化表示，这个期望符号E同时包含了环境和soft策略的混合作用，我们以`$\epsilon$`概率随机产生动作，`$1-\epsilon$`概率选择q值对应最大动作，所以求期望的作用包含2个：选择最大值情况和随机情况：  

```math
v_{*}(s)=\sum_{a}\frac{\epsilon}{|A(s)|}q_{\pi*}(s,a)+(1-\epsilon)\max_{a}q_{\pi*}(s,a)\\
=\sum_{a}\frac{\epsilon}{|A(s)|} \sum_{s',r}p(s',r|s,a)[r+\gamma v_{*}(s')]  
+(1-\epsilon)\max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_{*}(s')]    

```
可以发现上面2个表达式只在符号不同，所以当策略提升到极限时，就能达到最优策略，**当然这个最优是相比其他soft`$\epsilon$`-贪婪策略**。
## 5.基于重要度采样的离轨策略
>通过在线学习来实现控制面临一个困境，我们希望学习到最优动作，但是不得不为了学习而采取一些非最优动作来试探。同轨策略中行为策略和目标策略是一致的，学习一个最接近最优，且能保留试探能力的策略；离轨策略干脆把“学习最优”和“保持试探”两个目标分离，形成两个独立策略：行为策略及目标策略，行为策略负责交互训练时“保持试探”，目标策略用来形成最优策略，最终实现控制应用是用目标策略。

**覆盖假设下的预测问题**：在本小节中，我们讨论当行为策略`$b$`与目标策略`$\pi$`**都不变**，且**策略均已知**时，如何用`$b$`下与环境交互获得的数据来估计`$\pi$`下的价值函数。为保证`$\pi$`下的价值函数可以被估计，应当保证`$\pi$`下发生的动作在`$b$`下也能发生，也即是**对于某个状态s在采用目标策略可能采取动作a，在采用行为策略同样也可能采取动作a**，这也称为**覆盖假设**。**为保证探索性，`$b$`通常是随机的**，但`$\pi$`可以是确定的，在训练完成后实际应用采用的是`$\pi$`。  
**用重要度采样方法估计`$V_{\pi}(S)$`**：  
>**蒙特卡洛法估计期望**：首先假设随机变量服从某个分布`$X \sim P$`，概率密度为`$p(x)$`，计算随机变量的一个函数`$f(X)$`的期望`$E_{X \sim P}[f(X)]$`。  那么由概率密度积分形式有`$E_{X \sim P}[f(X)]=\int_{x}^{}f(x)p(x)dx$`。用蒙特卡洛积分计算形式有：`$E_{X \sim P}[f(X)]\approx \frac{1}{N}\sum_{i=1}^{N}f(\hat{x}_{i})$` ，其中`$\hat{x}_{i}$`是对随机变量X进行随机采样后获得的N个样本，N足够大。  
> **重要度采样**：现在假设这个随机变量也可能服从的分布是Q，其概率密度`$q(x)$`，同时我们还是认为X更可能服从P分布，所以我们还是要计算的是在P分布下的期望，写成可约分形式有：  
> ```math
> E_{X \sim P}[f(X)]=\int_{x}^{}f(x)\frac{p(x)}{q(x)}q(x)dx
> ```
> 我们现在按照服从Q分布进行大量均匀采样，得到数量巨大的N个样本`$x_{i}$`，用蒙特卡洛积分形式同样可以得到：  
> ```math
> E_{X \sim P}[f(X)]=\frac{1}{N}\sum_{i=1}^{N}f(\hat{x}_{i})\frac{p(x_{i})}{q(x_{i})}

> 其中`$\frac {p(x_{i})}{q(x_{i})}$`就是重要度权重。**总结：要计算分布P下的期望，如果对难均匀采样，那么选取一个相对容易的分布Q进行大量采样，然后计算**  

在强化学习中几乎所有的离轨策略都涉及到**重要度采样**方法，从起始状态`$S_{t}$`出发，在策略`$\pi$`下通过执行动作和转移状态产生了一条轨迹`$\tau :S_{t},A_{t},S_{t+1},A_{t+1}...S_{T}$`,从起始状态`$S_{t}$`开始产生轨迹`$\tau$`的概率为：  

```math
Pr \left \{ A_{t},S_{t+1},A_{t+1}...A_{T-1},S_{T}|S_{t}, A_{t:T-1}\sim \pi \right\}\\
=\pi(A_{t}|S_{t})p(S_{t+1}|S_{t},A_{t})\pi(A_{t+1}|S_{t+1})p(S_{t+2}|S_{t+1},A_{t+1})...\pi(A_{T-1}|S_{T-1}))p(S_{T}|S_{T-1},A_{T-1})\\
=\prod_{k}^{T-1}\pi(A_{k}|S_{k})p(S_{k+1}|S_{k},A_{k})
```
>完整的状态转移概率应该是依据贝叶斯公式，但是这是MDP所以只要三元组转移概率即可。Pr式子中条件`$S_{t}, A_{t:T-1}\sim \pi$`前面表示从某一特定初态出发，后面表示序列中动作的产生是依据策略`$\pi$`的分布。
略`$b$`

用上面的重要度采样方法，我们先来看下状态价值计算公式：  

```math
v_{\pi}(s) = E_{\tau \sim \pi}(G_{t}|S_{t}=s)
```
状态的回报`$G_{t}$`包含了未来的预测，预测依据是从St开始到终止态的轨迹，因此可以认为`$G_{t}$`是从St开始的**轨迹的函数**，而轨迹由于策略的因素就可以理解为一个随机变量。按照上面的**重要度采样**方法，我们可以通过采样策略`$b$`生成完整轨迹，其中每个状态`$S_{t}$`开始到终态`$S_{T}$`的子轨迹`$\tau$`可计算函数`$G_{t}$`，最终目标是求`$G_{t}$`的在策略`$\pi$`下的期望。  
**换句话说就是：我们知道在应用阶段控制器输出的“目标轨迹”分布满足P(目标策略作用)，要计算“目标轨迹”的函数的期望（值函数），但目标策略不与环境交互所以难采样，我们还知道这个“目标轨迹”分布应该和分布Q（行为策略作用）差不太多，而行为策略在训练时负责与环境交互，因此可以用行为策略采样，使用上面的重要度采样蒙特卡洛估计算法去计算轨迹函数的期望随机变量轨迹的函数。**  
**重要度权重** `$\frac {p(x_{i})}{q(x_{i})}$`中的概率密度比在这里就可以认为是**轨迹的概率密度**之比，对于表格型问题就是某条特定轨迹的概率，因此重要度权重由以下定义：  

```math
\rho_{t:T-1}\doteq \frac{\prod_{k}^{T-1}\pi(A_{k}|S_{k})p(S_{k+1}|S_{k},A_{k})}{\prod_{k}^{T-1}b(A_{k}|S_{k})p(S_{k+1}|S_{k},A_{k})}\\
=\frac{\prod_{k}^{T-1}\pi(A_{k}|S_{k})}{\prod_{k}^{T-1}b(A_{k}|S_{k})}
```
>`$\rho_{t:T-1}$`描述了在目标策略和行为策略下产生某条特定轨迹的概率之比，可以看出这个比值与MDP模型状态转移概率无关，只与采样的轨迹以及策略本身有关。下标代表了计算这个比值需要参考第t时刻状态到第T-1时刻的轨迹数据。


现在我们可以用策略`$b$`采样生成的轨迹来估计策略`$\pi$`下的状态价值`$v_{\pi}(s)$`了，依据重要度采样公式：
```math
E_{X \sim P}[f(X)]=\int_{x}^{}f(x)\frac{p(x)}{q(x)}q(x)dx=\frac{p(x)}{q(x)} E_{X \sim Q}[f(X)]
```
我们把状态值函数关于回报期望的公式改写成：  

```math
v_{\pi}(s)=E_{\tau \sim b}[\rho_{t:T-1}G_{t}|S_{t}=s]
```
相当于用策略b估计值函数，但是回报需要加上一个**权重**`$\rho_{t:T-1}$`。  
考虑在一个分幕式任务中，我们的行动策略b生成了**一连串轨迹**（指起点到每幕的终点），用t表示**全程的某个时刻**（这样不论那一幕的时刻都会有个独立的编号，不再区分幕数）。定义轨迹`$\tau$`是一个字典类型的数据结构，键值为状态，出现的时刻编号为值，即`$ \left \{ s_1:t, s_2:t+1 ...\right \}$`（如果是首次访问型，那么只会记录下状态第一次出现的时刻）,定义`$\tau (s)$`是**状态s在全程所有幕中出现时刻的集合**。`$T(t)$`表示时刻t后遇到的第一个终止态，也就是时刻t所在幕的结束状态。`$G(t)$`代表**从t时刻到T(t)分幕终止时刻的回报**，由于每一幕中某个状态s都有可能被访问到，因此组成了**集合**`$\left\{G(t)\right\}_{t \in \tau (s)}$`。如果我们关注于**某个状态s**下的回报，那么**状态s的回报值取值集合**就是`$\left\{G(t)\right\}_{t \in \tau (s)}$`，对应的为了求状态s回报的期望，我们需要计算的重要度采样权重写成` $\left \{\rho_{t:T(t)-1} \right \}_{t \in \tau (s)}$`。
> 注意：行动策略b实际操作时是可变的，甚至在每个时刻都随时变化的，因为是基于采样的蒙特卡洛方法，且策略b作为已知，只要保证覆盖性条件即可。


**普通重要度采样、加权重要度采样**:  
**普通重要度采样**就是依据下面的式子通过简单平均求策略b下的轨迹的回报期望就可以用作值函数的估计：  

```math
v_{\pi}(s)=E_{\tau \sim b}[\rho_{t:T-1}G_{t}|S_{t}=s]\\
\approx \frac{\sum_{t \in \tau(s)} \rho_{t:T(t)-1}G_{t}}{|\tau(s)|}

```
可以看出这就是前面蒙特卡洛估计法给每个回报加上了一个增益，从重要度采样公式来说这是一个**无偏估计**，但在实际应用中，两个策略（`$\pi$`和`$b$`)生成完全相同的轨迹概率非常小（即“离轨度”不低），导致权重变得极大或极小（方差大），需要非常长的轨迹（或者说采样样本空间）才能达到好的评估效果。为了“离轨”导致缓解方差大的问题，把求平均变为加权平均是一个办法：  
```math
v_{\pi}(s)\approx  \frac{\sum_{t \in \tau(s)} [\rho_{t:T(t)-1}G_{t}]}{\sum_{t \in \tau(s)} \rho_{t:T(t)-1}}

```  
这样每个时刻的回报的权重变为`$\frac { \rho_{t:T(t)-1}}{\sum_{t \in \tau(s)} \rho_{t:T(t)-1}}$`，该方法就是**加权重要度采样**法。虽然方差减小了，但这其实是一个**有偏估计**，因为如果我们考虑首次访问型方法，在一幕结束后开始查找轨迹字典来更新该幕中涉及状态的值函数，由于状态出现的时间只有一次，所以公式中求和符号全部取消，分子分母权重约掉，相当于用当前策略（动作策略）的回报来估计目标策略的价值，这显然是一个有偏估计。在实际应用时，加权重要度采样用的会多一些。
>**如何理解采样轨迹中“加权回报”的方差会出现无穷**？考虑一个简单的只有两个状态和一个动作（向左或向右）的问题，包括一个起始状态和终止状态，目标策略`$\pi$`在起始状态的策略是只向左，行为策略`$b$`则是各有50%几率向左或向右，向左后有10%几率进入终止状态，而有90%几率又回到起始状态，达到终止状态时奖励为1，其他都是0。根据方差定义：  
> 
> ```math
> Var[X]\doteq E[(X-\bar{X})^2]=E[(X-E(X))^2]=E[X^2-2XE(X)+(E(X))^2]\\
> =E(X^2)-(E(X))^2=E(X^2)-(\bar X)^2
> ```
>由于回报均值肯定是有限的，所以现在考察采样“加权回报”平方的期望：  
> ```math
> E_{b}[(\prod_{t}^{T-1}\frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}G_t)^2]
>```  
>对于此问题只有一个非终止状态所以`$ G_t = G_0 $`，如果采取向右的动作，由于`$\pi$`中无此动作所以所涉及期望项一定为0，现在考虑向左的动作获取回报的情况，向左动作后存在多步来跳回原状态的情况（向左后向右会使分子出现0故不需要考虑），其“加权回报”计算表达式为：  
> 
> ```math
>  E_{b}[(\prod_{t}^{T-1}\frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}G_0)^2]\\
>  =\frac{1}{2}*0.1*(\frac{1}{0.5})^2向左，结束\\
> +\frac{1}{2}*0.9*\frac{1}{2}*0.1*(\frac{1}{0.5}*\frac{1}{0.5})^2向左，返回，向左，结束\\
> +\frac{1}{2}*0.9*\frac{1}{2}*0.9*\frac{1}{2}*0.1*(\frac{1}{0.5}*\frac{1}{0.5}*\frac{1}{0.5})^2向左，返回，向左，返回，向左，结束\\
> +...\\
> =0.1*\sum_{k=0}^{\infty}*0.9^k*(2^{k+1})^2*(\frac{1}{2})^{k+1}\\
> =0.2*\sum_{k=0}^{\infty}1.8^k=\infty
> ```
>这样，轨迹、或者说加权回报的方差就是无穷。
## 6.离轨策略下蒙特卡洛预测法增量式更新实现  
>新平均=旧平均 + 步长*(新加入元素-旧平均)  

**普通重要度采样下的更新方法**：  
```math
Q_{n+1}=Q_{n}+\frac{1}{n}\left[\rho G_{n}-Q_{n}\right]
```
对于特定状态动作对，每当试验中出现该状态动作对时，记录下从其出发到终止态的回报，并乘以此条轨迹的采样权重修正，这就可以用增量式求平均法迭代出Q的值。其中`$\rho$`是重要性权重。  
**加权重要度采样下的更新方法**：  
书中以估计某一状态的价值为例，估计Q也是类似的。假设将分幕式任务时刻认为是连续的，`$G_1,G_2,G_3,...G_{n-1}$`是整个过程中从同一状态（如s）出发到各自幕终止态轨迹上该状态的回报，更新在每幕结束后进行（完成一幕后发现该状态就对该状态计算回报），那么根据上面的加权重要度采样法，我们定义下面这个更新规则：  

```math
V_{n} \doteq \frac{W_1G_1+W_2G_2+...W_{n-1}G_{n-1}}{W_1+W_2+...W_{n-1}}\\
=\frac{\sum_{k=1}^{n-1} W_{k} G_{k}}{\sum_{k=1}^{n-1} W_{k}} 
```  
下标n代表是第n步计算步骤，下标k代表在整个过程中出现待估计状态的序号。在加入新的回报`$G_n$`用增量更新公式可以写成：  

```math
V_{n+1} \doteq V_{n}+\frac{W_{n}}{C_{n}}\left[G_{n}-V_{n}\right]
```
这里`$C_n$`是原先所涉及轨迹的重要度权重再加上新增的`$G_n$`所在轨迹的重要度权重，即：  

```math
C_{n+1}=C_{n}+W_{n+1}
```
**加权重要度采样的离轨策略MC策略评估算法**  
和前面的试探性出发假设下蒙特卡洛方法一样，都是后面出现的状态回溯来进行更新对应二元组的值函数，但不同的是加上了权重，另外离轨策略条件下若出现行为策略中不可能出现的采样值需要判断并舍弃该条轨迹。
```
离轨MC策略评估，加权重要度采样，用于估计q_{\pi}
输入：待评估目标策略
初始化：Q(s,a),C(s,a)
循环生成多幕{
    确定一个满足覆盖假设的行为策略b #注意：每幕这个策略可以是不同的
    依据b生成一幕序列:S0,A0,R1,S1,A1,R2,...S_{T-1}, A_{T-1}, R_T, S_T
    G=0
    W=1
    if W!=0 
        for t=T-1,T-2,...0         #如果W!=0,对该幕序列中每个时间点进行循环处理
            G=gamma*G+R_{t+1}
            C(S_t, A_t)+=W
            Q(S_t, A_t)+=(W/C(S_t, A_t)) * [G-Q(S_t, A_t)]
            W=W * ( pi(A_t|S_t)\b(A_t|S_t) )
            if W==0
                break
            end if    
        end for    
    end if        
}       
```
>W=0时说明采样中出现了目标策略中不可能出现的情况，因此该幕轨迹不能参与计算值函数，直接作废去考虑生成下一幕。 

## 7.离轨策略下蒙特卡洛控制
本节我们将前面的基于soft-`$\epsilon$`贪婪策略的策略提升与离轨策略下的价值评估结合起来，形成一个**价值迭代**算法。目标策略`$\pi$`是待提升改进的策略，对于表格型问题只需贪婪的选择q(s,a)对应最大的动作，行为策略`$b$`需要被设计成soft策略，满足覆盖性条件并保证探索度。这里值得注意的是，**行为策略`$b$`可以随时变化，只要其策略可知并满足soft以及覆盖性条件，价值迭代仍能够使目标策略`$\pi$`收敛到最优策略`$\pi*$`**。下面是其伪代码：  

```
离轨策略下的MC控制算法，使pi收敛到pi*
初始化：Q(s,a),C(s,a)
初始化目标策略：pi(s)=argmax_{a} Q(s,a)
循环生成多幕{
    行为策略b设置为一个参数任意的soft-epsilon策略
    依据行为策略b采样生成一幕序列:S0,A0,R1,S1,A1,R2,...S_{T-1}, A_{T-1}, R_T, S_T
    G=0
    W=1
    for t=T-1,T-2,...0         #对该幕序列中每个时间点进行循环处理
        G=gamma*G+R_{t+1}
        C(S_t, A_t)+=W
        Q(S_t, A_t)+=(W/C(S_t, A_t)) * [G-Q(S_t, A_t)]
        pi(S_t)=argmax_{a} Q(S_t,a)   #上步价值估计完后立即策略改善
        if A_{t} != pi(S_t)           #如果发现采样时动作并不是本次提升后目标策略下的动作，直接舍弃该幕轨迹，重新生成一幕
            break
        W=W * ( 1\b(A_t|S_t) )
    
}
```
> 在上面的代码中，W更新不再是策略评估中的：  
> 
> ```math
> W \leftarrow W \frac{\pi\left(A_{t} \mid S_{t}\right)}{b\left(A_{t} \mid S_{t}\right)}
> ```
> 这是因为策略改进是贪婪的，所得的目标策略一定是一个确定性策略，即`$\pi\left(A_{t} \mid S_{t}\right)=1$`，这与前面的目标策略评估不一样，因为目标策略没有指定是确定性的还是概率性的。

为何在策略提升后如果发现该状态下“最优动作”与采样时的动作不一致，就舍弃整幕轨迹呢？书中没有解释，个人理解是目标策略`$\pi$`是确定性策略，只要提升后的`$\pi_{new}$`与行为策略`$b$`的决策结果不同就认为是生成了两条不同轨迹，这样通过`$b$`的采样无法重要度采样法评估`$\pi$`下的值函数，W直接归0，没有继续往下做的必要了，故舍弃。  
另外这段代码的算法只在每幕结束后更新，且还是从后往前更新，带来的一个问题是如果是行为策略表现的不那么"贪婪"，尤其是这种现象出现在轨迹后半段时，程序会舍弃大量的分幕轨迹数据，从而降低学习效率。