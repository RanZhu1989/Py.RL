# 第4章 动态规划  
> 动态规划Dynamic Programming DP在优化问题中指，在给定一个用MDP描述的完备环境模型的情况下，可以计算出最优策略。由上节中价值函数的两个贝尔曼公式，对于`$V_{*}(s)$`求出状态s下对应的a，对于`$q_{*}(s,a)$`则是求在当前给定状态动作二元组(s,a)下，对于每一个可能的后续状态s'时所采取的动作a'  

## 1.策略评估-计算值函数 
策略评估就是计算在某一特定策略`$\pi$`下MDP问题中各状态的价值函数`$v_{\pi}(s)$`，对于某一状态s的价值函数，有**贝尔曼公式**：  

```math
V_{\pi}(s)=\sum_{a}\pi(s|a)\sum_{s',r}p(s',r|s,a)(r+\gamma V_{\pi}(s'))  
```  
要求出每个状态s的价值函数，就是联立方程组求解一个|s|维线性方程组。而解这个方程组有些困难。下面我们将基于上面的公式，采用迭代方法，第k+1次的状态价值可由第k次状态价值迭代计算得。这实际是一种**自举**方法。  

```math
V_{k+1}(s) \doteq \sum_{a}\pi(s|a)\sum_{s',r}p(s',r|s,a)(r+\gamma V_{k}(s'))  
```  
由于这种迭代实际是基于已知策略与环境下的期望，与所有后一状态s'有关，又被称为策略评估的**期望更新**。同样的，用这种迭代方法在计算状态s下的值函数时需要遍历所有`$s\xrightarrow[]{a}s'$`  

**同步更新和异步更新法**：   
在进行值函数更新时，分为**同步更新**和**异步更新**，同步更新需要设定两个变量数组Vk+1和Vk，分别储存**新值函数**和**旧值函数**，每次更新时由++状态s'的旧值函数++的更新++状态s的新值函数++。而异步更新只有一个用于存储值函数的数组变量，在每次更新时可以由++状态s'的新值函数++的更新++状态s的新值函数++

**策略评估算法伪代码**：
```
输入：待评估策略，阈值theta>0 很小
每个v(s)赋初值
while(theta > 0)
  delta = 0
  for s in S
      v = v(s)
      v(s) = 期望更新公式
      delta = max { delta,|v-v(s)| }
      if delta < theta
      break
  End for
End while  
```

## 2.策略改进（策略提升）-寻求更优策略
**策略改进的思路**：  
对于策略`$\pi$`在状态s时，若不按原策略`$\pi$`选取动作而是按照新策略`$\pi'$`选取**新动作**，但在**后续继续使用**策略`$\pi$`选取动作。这种试探行为可以在状态s时是否存在更优的动作。这个思路也就是前面**最优动作-状态价值函数**的求取方法（*参考第三章中相关内容*）。在选取了新动作a后，将`$q_{\pi}(s,a)$`与`$v_{\pi}(s)$`进行比较，如果`$q_{\pi}(s,a)>v_{\pi}(s)$`，则说明在状态s下选择的新动作a比原策略要好，用策略改进定理可以证明。   
**策略改进定理**：  
若`$\pi$`和`$\pi'$`是两个确定的策略，`$\pi'(s)$`表示依据策略在状态s时映射的一个动作。对于任意动作s都有：

```math
q_{\pi}(s,\pi'(s))\geq v_{\pi}(s)
```  
那么则称策略`$\pi$`和`$\pi'$`一样好或者`$\pi'$`比`$\pi$`更好。  
下面从上面的不等式开始证明为何该关系存在时，新策略表现不会差于旧策略。  

```math
v_{\pi}(s) \leq q_{\pi}(s,\pi'(s))=E_{\pi}[R_{t+1}+ \gamma v_{\pi}(S_{t+1}) |S_{t+1}=s',A_{t+1}=a']\\
\leq E_{\pi}[R_{t+1}+ \gamma q_{\pi}(s', \pi'(s'))]\\
=E_{\pi}[R_{t+1}+ \gamma E_{\pi'}[R_{t+2}+ \gamma v_{\pi}(S_{t+2})]]\\
\leq E_{\pi}[[R_{t+1}+ \gamma E_{\pi'}[R_{t+2}]+ \gamma^2 E_{\pi'}[R_{t+3}]+...]\\
=E_{\pi'}[R_{t+1}+ \gamma R_{t+2}+ \gamma^2  R_{t+3} ]\\
=v_{\pi'}(s)
```  
**更优策略的产生**  
从前面的分析可以看出，只要给定一个策略及价值函数，就可以评估在该状态s下改变一次动作会产生怎样的后果，将这个思路扩展到所有的状态和动作，在每一个状态s下选取一个最优动作a，即考虑一个贪婪策略`$\pi'$`满足:

```math
\pi'(s) \doteq argmax_{a} q_{\pi}(s,a)\\
=argmax_{a}\sum_{s',r}p(s',r|s,a)(r+ \gamma v_{\pi}(s'))
```  
不断单步向前搜索，看似“贪婪”，但这样得到的新策略至少和以往相比一样好，甚至更优，这称为**策略改进**，对应值函数形式就是下面的求**最优值函数**问题。
> 如果有多个动作使状态-动作值函数取到最大值，那么在设定新策略时只要确保“除这些最优动作外其他动作”的概率都为0即可。

```math
v_{\pi'}(s)=\max\limits_{a} \sum_{s',r}p(s',r|s,a)(r+\gamma V_{\pi'}(s'))\\

```  
通过策略改进方法把所有状态下的值函数全部遍历一遍，这样`$\pi'$`就会变成一个“**更优**”的策略。  

## 3. 策略迭代-最优策略的形成  
在上面的过程中，我们每次选择新动作时参考了原策略条件下，下一个状态的价值，从形成了新策略。而新策略下的值函数还需要重新评估，通过在新策略下尝试新动作，不断对策略“迭代升级”来最终收敛到一个**最优策略**，这种方法称为**策略迭代**

```math
\pi_{0}\xrightarrow[]{策略估计}v_{\pi_{0}}\xrightarrow[]{策略改进}\pi_{1}\xrightarrow[]{策略估计}v_{\pi_{1}}\xrightarrow[]{策略改进}\pi_{2}...
```
下面给出策略迭代形成最优策略算法的伪代码：  

```
1.形成初始策略pi
2.进行策略评估，计算出每个状态的v(s)
3.进行策略改进
policy-stable = true
k=0
for s in S
    old-action = pi(s)
    pi(s)=策略改进公式
    if old-action != pi(s)
       policy-stable = false
    else 
       k++
    end if   
    if  k==|S|
        return v* pi* #返回最优价值函数及策略
    else
    pass
    end if
    if policy-stable == true
       return v* pi*
    else
    GOTO 2 #到第二步再进行新的策略评估，直至收敛
    end if
end for
```
## 4.价值迭代-截断式策略迭代 
在前面的策略迭代算法中，需要对状态遍历实现策略评估，对于状态空间非常大的问题，耗时是不可接受的。由于策略评估实际上是针对某一策略迭代求出价值函数，在使用贪婪策略选取动作时我们**更关注价值的相对大小而不是绝对大小**（在书中格子世界示例中已看到经过3次迭代后策略就基本成型了）。我们在进行价值迭代时会用到这些价值函数，所以一个想法是：用经少量迭代后“不精确”的价值函数再进行策略迭代，把策略提升的工作**转移到后面策略迭代上来**。*策略的提升最终反应在选取动作的价值上*。下面是将**最优价值函数**的贝尔曼公式改写成迭代形式：  
```math
v_{k+1}(s) \doteq \max \limits_{a}\sum_{s',r}p(s',r|s,a)(r+\gamma v_{k}(s'))  
```  
式中的max a提供了策略提升功能，后面的部分是策略评估，只要环境已知，价值函数总能收敛到v*，该公式max后的部分已经具备了策略评估的功能。  
**价值迭代算法，目标求出**`$\pi*$`，这个策略实际是一组状态动作序列。
```
输入：阈值theta>0 很小
每个v(s)赋初值
while(theta > 0)
  delta = 0
  for s in S
      v = v(s)
      v(s) = 最优价值函数的贝尔曼公式迭代形式
      delta = max { delta,|v-v(s)| }
      if delta < theta
      记录该状态动作对(s,a)
      break
  End for
End while  
输出：整个状态动作对序列，作为策略
```
## 5.异步动态规划  
在1.中已经讲到，如果问题的状态空间很大，更新一个状态需要涉及巨量其他状态。在前面已经讲过单变量数组储存值函数的情况，另外可以单独只更新一些我们关注状态的值函数。
## 6.广义策略迭代GPI  
通过策略评估以及策略改进相互作用形成趋向于最优策略和最优值函数的过程称为广义策略迭代，几乎所有的强化学习方法都可以描述为广义策略迭代。   
下面这张图借用知乎上一张图， 比书上更准确，因为策略迭代一开始估计的值函数不是真实的
![image](https://pic4.zhimg.com/80/v2-4cbd1c0d42fe5c2be5abf6173e4f0765_720w.jpg)