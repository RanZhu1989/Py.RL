# 强化学习三条主线：
1. 动物心理学
2. 最优控制
3. 时序差分
# 第2章 多臂赌博机问题
## 1.赌博机
**任务**:   
在k个选项中每次选择一个执行，每次选择后获得一定收益。**这次选择不会对下一次选择产生影响**   
即存在k台老虎机，每次只能选择玩一台老虎机，每个老虎机都有自己的运行机制，每次我们玩这台老虎机都能有概率赢钱或输钱，并且倍率可能还不一样。对于每台老虎机，每次玩都会存在一个期望收益（赢多少钱或输多少钱）。  
**即时收益**:  
t时刻动作At获得的**收益**记为Rt  
**动作的价值**：  
动作a对应的价值记作`$q_*(a)$`,动作的价值就是动作产生收益的**期望**，在这个问题中我们认为“选择玩一台老虎机”这个动作的价值就是玩这台老虎机赢钱的期望。
```math
q_*(a)=E[R_t|A_t=a]  
```  
这个表达式描述的是动作**真实价值**    
**价值的估计**：  
老虎机收益期望是未知且客观存在的，在这个问题中，我们需要估计每个动作的真实价值，离真实值越接近越好  
**动作的选取-贪心\探索**：  
如何选择动作？如果老虎机内部代码开源，那么我们就能计算出每台老虎机真实的收益期望，每次选择最大的就行了。但我们现在只能依据估计的价值来作为动作选择的依据。无论我们用何种方法来估计价值，我们总会找到估计价值最高的那台老虎机，这些估计价值最高的动作就称为**贪婪动作**。如果我们选择贪婪动作，就是**开发**。如果我们选择非贪婪动作，就是**试探**，开发与试探是存在冲突的。  
## 2.动作-价值 方法
动作的真实价值就是执行这个动作所产生的期望收益。用动作的价值来决策动作的选择就称为动作-价值方法。  
**平均价值估计法**：  
最简单的估计一个动作价值的方法是求这个动作价值的历史平均值，即把前面时刻的收益加起来，再除以动作已执行的次数。这个方法并不是最好的方法。  
**贪婪策略**：  
贪婪策略就是选择最高估计价值的动作，如果存在多个动作的估计价值都是最高的，那么就随机选择一个。
**`$\epsilon$`贪婪策略**：  
`$\epsilon$`是贪婪的概率，一般较大，即大部分时候选择贪婪动作进行开发，偶尔会进行探索。  
**非平稳性**：  
在许多时候环境会发生变化，一个动作的真实价值也随即发生变化。  
**增量式价值估计法**：  
一般用：新估计价值=旧估计价值+步长*[新加入收益-旧估计价值]，这是一个很重要的形式  
++**平均价值估计的增量式实现**++，第n次迭代的步长为`$\frac{1}{n}$`，一个平稳问题只要次数上去了，依据大数定律都能收敛到真值  
++**对于非平稳问题，一个常用的方法是步长选取为一个固定参数**++，这个求得的是历史数据的加权平均，相对于上面的完全平均估计而言，越新加入收益的权值越高，可以看下面的公式  

```math

Q_{n+1}=Q_n+\alpha[R_n-Q_n]\\=(1-\alpha)^nQ_1+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-i}R_i

```  
由于`$\alpha<1$`，对于第n次迭代，越久远的收益数据相对较近期的收益，它的权值更低。  
**乐观初始价值**：  
在开始迭代式估计价值时，我们最好能够遍历到每一种动作去收获收益，这样很快消除偏差。但在上面所述的贪婪方法中，设置高的估计价值的初值会影响开始阶段的动作选择。举个例子：如果真实价值为平均值为0，方差为1的正态分布，而我们把初值都设置为5，这样Agent在试探动作时会被初值引导去尝试其他动作，即进入一种强制试探状态。
## 3.UCB 基于置信度上界的动作选择法  
在贪婪策略中，只依据最大价值来选取动作，不会关心那些接近最大价值的动作，换句话说可能这些动作真实的潜力更大，只是受制于估计方法导致某一步没有取得最大值。UCB按照以下公式选取动作：  
```math
A_t={\arg\max}_{a}[Q_t(a)+c\sqrt{\frac{\mathbf {ln}t}{N_t(a)}}]
```  
其中，c是一个大于零的常数，t为当前时刻，`$N_t(a)$`为在t时刻之前动作a的数量。如果`$N_t(a)=0$`，a就被认为是最优动作输出了。开方的那一项描述的是动作a价值的不确定性，当选中动作a时分母增加，不确定性减小了，若没有选中a，分子还是会增加，不确定性增大了，但In比分母加一相比增加会很小，随着时间的推移不确定性增加会越来越小，最终能够迫使算法不停选择所有的动作，但被选择过的以及价值很低的动作后面被再次选择的可能性会很低。  
## 4.赌博机的梯度算法
梯度算法不关心哪个动作的价值最大，在这个问题中对于每个动作a，考虑如何学习一个**偏好函数**`$H_t(a)$`，偏好函数越大，该动作越有可能被选中。如果定义动作a在t时刻被选择的概率按玻尔兹曼分布（用于离散动作状态空间或者动作状态空间不大的情形）表示`$\pi_t(a)$`如下：

```math
\pi_t(a)=\frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}
```  
书中定义了下面的梯度上升方法，在执行动作`$A_(t)$`并获取收益后，需要与收益基准值`$\overline R_t$`比较，其中a是除当前执行动作`$A_(t)$`外其他所有未执行的动作，`$\alpha$`是一个大于零的步长。

```math
H_{t+1}(A_t)=H_t(A_t)+\alpha(R_t-\overline R_t)(1-\pi_t(A_t))

H_{t+1}(a)=H_t(a)-\alpha(R_t-\overline R_t)\pi_t(A_t)
```
可以看出：
如果执行当前动作的收益大于基准收益，那么该动作的偏好函数数值会增加，而其他动作偏好函数值会减小，这样在`$\pi_t(a)$`作用下后续选择该动作的概率会增加，反之如果低于基准收益，该动作概率会减小，而选择其他动作的概率会增加。  
**方法由来**  
研究偏好函数`$H_t(a)$`对系统性能的影响，这里系统性能指收益的总体期望（对于所有动作）
```math
E[R_t]=\sum_{x}\pi_t(x)q_*(x)
```  
随后描述系统性能对偏好函数的影响：

```math
H_{t+1}(a)=H_{t}(a)+\alpha\frac{\partial E(R_t)}{\partial H_t(a)}
```  
下面开始把上面第一个式子带到第二个式子中，研究偏导项：  

```math
\frac{\partial E(R_t)}{\partial H_t(a)}=\frac{\partial\sum_x\pi_t(x)q_*(x)}{\partial H_t(a)}
=\sum_x\pi_t(x)(q_*(x)-\overline R_t)\frac{\partial\pi_t(x)}{\partial H_t(a)}\frac{1}{\pi_t(x)}
```  
为何无故增加一项？对于每个动作的被选中的概率加起来都是1，且不论策略如何变化，其概率变化率之和都是0。对应上面的公式中`$\sum_x\frac{\partial\pi_t(x)}{\partial H_t(a)}$`这一项即变化率之和为0。
前面的`$\sum_x\pi_t(x)$`乘以后面的东西相当于对x个样本求期望（假设这个随机变量为`$A_t$`)，改写为期望形式：

```math
\frac{\partial E(R_t)}{\partial H_t(a)}=E[(q_*(A_t)-\overline R_t)\frac{\partial\pi_t(A_t)}{\partial H_t(a)}\frac{1}{\pi_t(x)}]
```   
回到本章一开始讲的：动作的价值定义为该动作收益的期望，即`$E[R_t|A_t]=q_*(A_t)$`。某一动作的真实价值是客观存在的，注意到前面已有求期望符号，所以这里我们直接用Rt来代替q*，变成：
```math
\frac{\partial E(R_t)}{\partial H_t(a)}=E[(R_t-\overline R_t)\frac{\partial\pi_t(A_t)}{\partial H_t(a)}\frac{1}{\pi_t(x)}]
```   
接下来可以由商的复合求导公式推出：

```math
\frac{\partial\pi_t(x)}{\partial H_t(a)}=\pi_t(x)[(\mathcal1_{a=x})-\pi_t(a)]
```  
其中`$\mathfrak 1_{a=x}$`表示当x=a时取1，否则为0。
最后我们综合以上，我们有：

```math
H_{t+1}(a)=H_{t}(a)+\alpha(R_t-\overline R_t)[(\mathcal1_{a=x})-\pi_t(a)]
```
Q.E.D  
## 5.从单情景的赌博机到多情景关联的赌博机问题
上面我们提到了两个概念：  
**平稳赌博机任务** - k台赌博机内置游戏程序是固定的  
**非平稳赌博机任务** - k台赌博机内置游戏程序是变化的，这种变化不为观察者所知  
在**单情景任务**中，即使我们知道赌博机内置程序是变化的，每次决策时也只是在**跟踪**这个变化的内置程序。  如果赌博机的外观颜色代表了赌博机具有某种策略，假设赌博机内置程序改变时，它的外观颜色会变化，一个很自然的想法就是通过把外观颜色和当前赌博机策略下最优动作关联起来，这种被称为**多情景关类任务**，多情景关联任务介于单情景赌博机问题以及强化学习框架之间，因为动作只会影响即时收益，获取收益后本情景立即结束，不会影响未来收益。所谓的关联是指利用已识别的历史情景进行决策。
