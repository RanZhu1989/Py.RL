# 第6章 时序差分学习（TD）
> **MC方法**在每一幕结束后，依据采样来进行更新，需要形成从初始状态到终幕的多条采样轨迹，但优点是不需要知道系统的动力学模型；**DP方法**不需要走完整个轨迹而是通过自举更新，但是需要依据系统的动力学模型来确定轨迹演化方向。**TD时序差分**结合了MC与DP的特点，同样可以用采样数据学习，但是不需要等到走完整个轨迹，而是依据某个状态短时间窗口内的观测奖励和估计价值来更新该状态的价值，这个工作可以在轨迹演化的同时同步进行。时序差分四个字的意思是：更新当前时刻的值函数需要在后面的时刻进行 

## 1.时序差分预测  
### TD(0)方法:  
对于MC方法，从一个非终止态`$S_{t}$`出发采样出到终止态`$S_{T}$`的轨迹，在达到终止态后才进行值函数更新和策略改善。MC估计策略`$\pi$`下状态s的价值可以用以下的增量式方法：  

```math
V_{\pi}^{new}(S_t)\leftarrow V_{\pi}^{old}(S_t)+\alpha[G(S_t)- V_{\pi}^{old}(S_t)]
```
注意G(s)的计算需要走完全程才知，由于：  

```math
G(S_t)=R_{t+1}+\gamma G(S_{t+1})
```
现在我们不走完整个轨迹，即`$G(S_{t+1})$`项需要用估计值替代，而某状态下的回报G的期望就是V，因此有下面的式子：  

```math
G(S_t)=R_{t+1}+\gamma G(S_{t+1})\\
\approx R_{t+1}+\gamma E_{\pi}[G(S_{t+1})]\\
= R_{t+1}+\gamma V_{\pi}(S_{t+1})

```
这样MC下“需要走完全程”的值函数可以每走2步更新一次的方法来“实时更新”：  

```math
V_{\pi}^{new}(S_t)\leftarrow V_{\pi}^{old}(S_t)+\alpha [ R_{t+1}+\gamma V_{\pi}^{old}(S_{t+1})-V_{\pi}^{old}(S_t)]
```
其伪代码算法实现方式如下：  

```
表格型TD(0)时序差分预测算法，用于估计V_{\pi}
输入：待评估策略pi 步长alpha
初始化：V(S)表格，终止态V(S_T)=0
循环{
    选择一个起点s，从s开始对于每步:
    {
        a=pi(s)
        采样(s,a,s',r)直到终止状态
        对于每个遇到的状态V(s)=V(S) + alpha *[R +gamma*V(s')-V(S)]
        s=s'
    }
}
```
### TD误差：  
TD(0)状态值函数更新法中，更新的增量`$R_{t+1}+\gamma V_{\pi}(S_{t+1})-V_{\pi}(S_t)$`是计算`$V_{\pi}(S_t)$`的**估计误差**，前两项之和是一种相比原先值函数更好的估计（因为数据量增加了）。前两项`$R(S_{t+1})+\gamma V_{\pi}(S_{t+1}) $`是根据“单步预测”得到的`$G_t$`的估计值，而不是走完全程的实测值。我们用**TD误差**`$\delta_t$`来表示用t时刻回报`$G_t$`的估计值来更新值函数`$V_{\pi}(S_t)$`的误差：  

```math
\delta_t=R_{t+1}+\gamma V_{\pi}(S_{t+1})-V_{\pi}(S_t)
```
>TD误差虽然指当前时刻t更新V(s)时的误差，但它取决与后面产生的收益和后续状态，因此至少要等一步后才可以计算TD误差（TD(0)），即`$\delta_t$`至少在t+1时刻才能获得。  

对于MC这类只在每幕结束后才更新值函数的方法来说，每次新采样轨迹中的`$G_t$`与该t时刻对应状态的上次估计的值函数`$V(S_t)$`的差值（即MC更新误差）也可以用TD误差表示：  

```math
G_{t}-V\left(S_{t}\right)=R_{t+1}+\gamma G_{t+1}-V\left(S_{t}\right)+\gamma V\left(S_{t+1}\right)-\gamma V\left(S_{t+1}\right)\\
=R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)+\gamma G_{t+1}-\gamma V\left(S_{t+1}\right)\\
=\delta_t+ \gamma[G_{t+1}-V\left(S_{t+1}\right)]\\
=\delta_t+ \gamma[\delta_{t+1}+\gamma(G_{t+2}-V\left(S_{t+2}\right))]\\
=\delta_t+ \gamma \delta_{t+1}+ \gamma^2 \delta_{t+2}+...+\gamma^{T-1-t}\delta_{T-1}\\
=\sum_{i=t}^{T-1}\gamma^{i-1}\delta_t 
```
可以看出MC方法下走完全程后更新误差就是每步TD(0)误差之和，这是一个有界的误差。以上表示的前提是`$V\left(S_{t}\right)$`在被更新前保持不变，换句话说就是值函数更新只在某些特定点批量更新。对于TD方法来说，由于在每幕中每走几步就要更新一次所涉及的值函数（对于TD(0)来说每采样一次就更新一次值函数），所以当完成一条完整轨迹时，再回过头来看MC更新误差，由于`$V\left(S_{t}\right)$`已被更新数次，只有当`$V\left(S_{t}\right)$`更新幅度较小时，上式才近似成立。
>把TD理解为一种不需要走完全程、值函数更新在幕内完成的特殊MC方法，或者干脆认为是一种走了几步就结束的分幕更新的MC，但是超出幕范围的值函数不能在线采样计算而是通过策略仿真得到。  

## 2.时序差分预测方法的优势
### 与其他两类方法相比：  
TD相比于MC运用了在线的实时学习策略，而不需要等到该幕结束，因为一些任务每幕持续很长甚至是连续型的，另外由于MC在更新时存在大量的被折扣的回报，导致学习速率很慢。  
TD相比于DP则是不需要环境模型。  
### 收敛性：  
对于固定的策略`$\pi$`采用TD(0)方法均可收敛到`$V_{\pi}$`，前提是如果步长`$\alpha$`够小、步长参数满足随机逼近理论概率1收敛条件，并且是针对表格型问题或者广义线性函数近似的问题。  
### 学习速度：  
TD和MC哪一种方法在估计`$V_{\pi}$`上更快，目前尚无证明，实践中发现TD在随机任务上通常比步长为常量的MC方法更快。  
> 书中随机游走的示例各状态价值计算用状态价值的贝尔曼方程，解方程组即可  

## 3. TD(0)最优性  
待填 没看明白

## 4.Sarsa-同轨策略下的时序差分控制  
### TD(0)下的Q值更新：  
回忆状态-动作价值函数`$q(s,a)$`的贝尔曼方程：  

```math
q_{\pi}(s,a)=r(s.a)+\gamma \sum_{s'}p(s'|s,a)\sum_{a'}\pi(s',a')q_{\pi}(s',a')
```
在TD方法下，后面的状态转移概率求和项变为由采样观测得的确定项（状态已转移到s'），另外a'也应是已知的（策略已知）： 

```math
q_{\pi}(s,a)\approx r(s.a)+\gamma q_{\pi}(s',a')
```
这样这个贝尔曼方程的近似表达式就是TD(0)下的
由增量式更新法的**更新目标**（`$Q_{target}$`)，对于待估计的Q表`$Q(S_t,A_t)$`有：  

```math
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha [R(S_t,A_t)+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
```
> Sarsa名称的由来是因为更新`$Q(s,a)$`需要涉及一个五元组`$\{s,a,r,s',a'\}$`  

>`$A_{t+1}$`实际是由**当前策略**（`$Q_{old}$`为依据)**仿真**出的，真正运行**时t+1时刻输出的动作**`$A_{t+1}$`是由**更新后策略**（`$Q_{new}$`为依据)得到的。


### TD(0)下的Sarsa同轨控制：  
与MC版本的同轨控制类似，TD(0)版本的Sarsa只是不再在终止态完成Q值更新，而是在每进行一步后更新上一步采样涉及的Q值。On-Policy体现在更新Q-Table后立即用该Q-Table产生下一步输出动作，即target policy与behavior policy是一致的，通过更新和测试来在线“学习”出一个Q-Table。由于是同轨策略，为保证探索度，所以必须采用软策略。下面是该算法的伪代码：  

```
Sarsa 目标：求出Q*
参数: alpha epslion
初始化：Q表
循环生成多幕{
    初始化起点S
    依据Q表采用epslion贪婪策略选取动作A
    执行A，状态转移到S'
    在S'下再次依据Q表采用epslion贪婪策略选择动作A'
    Q(S,A) = Q(S,A) + Alpha *[ R(S,A)+ Gamma*Q(S',A')-Q(S,A) ]
    由新Q表采用epslion贪婪策略更新下一状态动作A'
    S=S'
    A=A'
    持续以上直到终止状态
}
```
### 有风的格子世界仿真（书中习题）：  
下面考虑书中一个格子世界的编程例子，存在一片区域使得智能体每步移动后停留在其中的智能体向上偏移1-2格（仅考虑一次作用），起点编号是30，终点编号是37，智能体越界或达到目标作为该幕终点，智能体每次可以有九个动作（八个方向外加停止），每次移动一格。
![image](34FD802456A54C80B77729CEDBD3A2F0)
![image](D0BC6907AE7440DEA23E8F5C7FA93DDA)  
在这个问题中，如果采用MC方法，可能存在一种情况使得状态不再转移且达不到终止状态：由于幕不结束则不更新，在风的作用下智能体在风强为1的区域向下运动就会停在原状态。
## 5.Q-Learning -离轨策略下的时序差分控制
>在Sarsa中，控制器输出的下一步动作`$A_{t+1}$`需要参照更新后的Q表格用软策略生成，保证了探索度但有时输出的并不是贪婪动作。在离轨策略下，我们可以用软策略保证输出动作时的探索度，而用完全贪婪策略使得表格中某一个状态动作对的值函数收敛到最优值函数。这就是Q学习的思路。

### Q Learning的Q值更新思路：  
在DP中我们讨论了最优Q函数的贝尔曼公式：  

```math
q_{*}(s,a)=\sum_{r',s}p(s',r|s,a)(r+\gamma \max\limits_{a’}q_{\pi*}(s’,a’))
```
在TD(0)中当前执行动作`$a$`与下一状态`$s'$`分别是依据当前behavior policy`$b$`和环境驱动获得的，因此上式中求和期望形式可去掉化为近似估计的形式：  

```math
q_{*}(s,a)\approx r(s,b(s))+\gamma \max\limits_{a’}q(s’,a’)  
```  
其中行为策略b是一个软策略，后一项代表了target policy（即贪婪策略）的作用。  
上节Sarsa中`$q(s,a)$`更新的目标是通过逐步采样“逼近行为策略下的Q值”，当然这个soft的行为策略能够保证策略提升，而在Q-Learning中更新目标是“形成一个target policy下的Q值”，且该target policy其实就是最优策略（因为每步我们选取了当前最优动作，这些最优动作组成了最优策略，参考DP中最优Q函数贝尔曼公式的由来）。因此Q-Learning的`$Q_{target}$`就是：  

```math
R(S_t,b(S_t))+\gamma \max\limits_{A_{t+1}}Q(S_{t+1},A_{t+1})  
```  
于是Q-Learning下的Q值更新公式为：  

```math
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha [R(S_t,A_t)+\gamma \max\limits_{A_{t+1}}Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
```
> >`$A_t$`由软策略`$b$`获得  
> >Sarsa中更新Q时所需的下一步S'和A'是需要依据策略(这里是`$\epsilon$`-贪婪)得出的： 
> 
> ```math
> Q(S_{t+1},\pi(S_{t+1}))
> ```
> >而Q-Learning是直接从Q表中选取一个使Q值最大的当前最优动作：  
> 
> ```math
> \max\limits_{A_{t+1}}Q(S_{t+1},A_{t+1})
> ```
> 

### Q-Learning算法实现：  
与Sarsa算法类似，但在更新Q时对于下一状态的动作Q估计是直接选取最优动作而不是依据某个具体策略。

```
Q-Learning 将target policy：pi演化成最优策略 pi*
参数: Alpha  epsilon
初始化：Q表
循环生成多幕{
    初始化起点S
    依据Q表采用epslion贪婪策略选取动作A
    执行A，状态转移到S'
    在S'下依据Q表，选择使Q值最大的动作A'
    Q(S,A) = Q(S,A) + Alpha *[ R(S,A)+ Gamma*Q(S',A')-Q(S,A) ]
    由新Q表采用epslion贪婪策略选取下一状态动作A'
    S=S'
    A=A'
    持续以上直到终止状态
}
```
### Sarsa与Q-Learning在线性能的比较：  
由于Q-Learning在线学习时行为策略是软策略，但目标策略更新与行为策略无直接联系，所以也就是说行为策略相比目标策略来说“更新不完善”，在线学习时Q-Learning或许会产生一些意外的输出（冒险）。而Sarsa属于同轨策略，行为策略就是在线学习的目标策略，即生成目标策略时考虑了行为策略（探索性），所以其输出会较为保守（避免意外输出）。当`$\epsilon$`逐步减小时Q-Learning和Sarsa都能收敛到最优策略。

## 6.期望Sarsa  
从前面可以看出，TD(0)方法中目标策略更新方式还是DP中这个状态-动作价值的贝尔曼方程：  
```math
q_{\pi}(s,a)=r(s.a)+\gamma \sum_{s'}p(s'|s,a)\sum_{a'}\pi(s',a')q_{\pi}(s',a')
```
在TD(0)方法中，行为策略b作用下s'已知，但后面涉及下一动作的期望需要依据**目标策略函数**来求：  

```math
Q_{target}=R_{t+1}+\gamma \sum_{a'} \pi(a'|S_{t+1})Q(S_{t+1},a')
```
这样Q函数的增量式更新方式变为：  

```math
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha [R_{t+1}+\gamma \sum_{a'} \pi(a'|S_{t+1})Q(S_{t+1},a')-Q(S_t,A_t)]
```
这就是**期望Sarsa学习算法**，而Sarsa与Q-Learning可以看作期望Sarsa的特殊形式：当我们认为将`$\pi$`下的一次采样作为期望时，就是Sarsa；当待更新形成的目标策略是一个贪婪策略时，意味着期望就是选取下一状态下价值最高Q值，这就是Q-Learning。  
从MC到TD四种方法演化总结如下图：  

![image](7A60F9C2D950417ABCED04A75736DD2E)
![image](539B8D271C79408C870CBEF941C9D629)
![image](B1462BB36C514EC8B2DD56EEBE077297)  
## 7.最大化偏差与Double Q-Learning  
### 最大化偏差：  
基于“贪婪”方法的强化学习都存在一个问题，当价值估计过高时也许会被引导到该“过高”的价值对应输出动作上，这是因为进行“贪婪”（最大化操作，包含纯粹的贪婪及软贪婪策略等）操作时，对于状态s，选取当前Q表中状态s下q值最大时对应的输出动作a。举个例子，如果某个状态s对应多个动作，执行这些动作后的奖励分布均值较低，但方差很大，会出现学习时该状态出发的动作收获奖励过高的“假象”，导致智能体更倾向错误的选取该动作。估计的q值与其真实q值的差称为**最大化偏差**。  
**最大化偏差**产生的原因是因为**只有一张Qtable**，虽然前面我们知道该估计Qtable最终会收敛到真实情况下的Qtable，但在前期采样数据有限的情况下，**少量数据的最大值只能代表该段数据空间的最大值，而去代表整个样本的最大值是会造成最大化偏差的。**  
### Q-Learning中真实价值函数的需求：   
Q-Learning的Q值更新公式为：  

```math
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha [Q_{Target}-Q(S_t,A_t)]\\
Q_{Target}=R_{t+1}+ \gamma \max_{a'}Q(S_{t+1},a')
```
增量式更新的目的是将输出“牵引”到`$Q_{Target}$`，因此`$Q_{Target}$`需要更接近真实价值，由于`$R_{t+1}$`是采样获得的，则实际"真实价值函数"需求的是`$\max_{a'}Q(S_{t+1},a')$`：首先需要一张“准确”的`$Q(S_{t+1},*)$`表，然后依据这张“准确”的表寻找`$S_{t+1}$`下最大的Q值并返回对应的动作作为最优动作。 
>准确来说，`$Q(S_{t+1},*)$`只能算是一张“分表”
### Double Q-Learning：  
前面已知道最大化偏差是由于只有一张Qtable造成的，这张Qtable既要准确反应状态动作对的价值，又要依据这张Qtable去选择动作，一旦某个环节出现准确性问题，那么Q学习效果将受影响。换句话说，**只要不用那张价值被高估Qtable去同时作为Q更新或动作选取的参考**，就能一定程度上缓解最大化偏差问题。  
在Q学习中，我们可以用两张Qtable去分别负责逼近准确状态动作价值、以及作为最优动作选取的参考表格。在Q学习过程中，对于轨迹每个状态-动作对采样点(s,a)，以50%概率随机被选取在两张Qtable上对应的Q值进行更新，但Q值更新中涉及的`$\max_{a'}Q(S_{t+1},a)'$`
项中的下一状态的Q值则是查找另一张Qtable上的Q值，而不是继续用这张Qtable。这样限制了Q学习中所用的Qtable某些值“高估”的速度，从而缓解了**小样本中偶然事件使某些Q值升高的问题从而导致不断强化对偶发高奖励的倾向，从而错误决策的问题**。
```
graph LR
轨迹上的采样点-->|0.5概率选取|更新Qtable_1 
轨迹上的采样点-->|0.5概率选取|更新Qtable_2
更新Qtable_1--> 下个状态的Q值查找用Qtable2
更新Qtable_2--> 下个状态的Q值查找用Qtable1
```
两张Qtable更新公式如下：  

```math
Q_1(S,A)=Q_1(S,A)+\alpha[R(S,A)+\gamma Q_2(S_{t+1},argmax_{a’}Q_1(S_{t+1},a'))-Q_1(S,A) ]\\
Q_2(S,A)=Q_2(S,A)+\alpha[R(S,A)+\gamma Q_1(S_{t+1},argmax_{a’}Q_2(S_{t+1},a'))-Q_2(S,A) ]

```
可以看出Double-Q学习总体计算工作量相比Q-Learning没有变化，而内存消耗扩大一倍，因为要维护两张Q表格。
### 算法伪代码：  

```
Double Q-Learning 使Q1->Q2->Q*
参数：Alpha epsilon
初始化： 两张Qtable：Q1、Q2
循环生成多幕{
    初始化起点S然后开始生成轨迹
    将Q1和Q2表相加合并，采用epslion贪婪策略参考这张合并表选取动作A
    执行A，状态转移到S'
    以50%概率选择一个Q表(Q1或Q2):Qx
    在S'下依据Qx表，选择使Q值最大的动作A'
    若选择Qx=Q1: 
    Q1(S,A) = Q1(S,A) + Alpha *[ R(S,A)+ Gamma*Q2(S',A')-Q1(S,A) ]
    否则：
    Q2(S,A) = Q2(S,A) + Alpha *[ R(S,A)+ Gamma*Q1(S',A')-Q2(S,A) ]
    S=S'
    持续以上直到终止状态
}
```
## 8.游戏、后位状态和其他特殊例子  
在书的第一章举了一个下五子棋的例子，在五子棋中由于存在两个玩家，在我方玩家进行一次动作后，棋盘上的棋局只能算是一个过渡态，因为不知道对手落子在何处，我们在决策时依据的是当对手下完子后的棋局，在书中被称为**后位状态**，该状态的价值函数就是**后位状态价值函数**，后位状态价值用于处理环境动态信息不完整问题，例如：

```
X - -    - - -         - - -      X - -
- O -  + - - X   OR    - O X  +   - - -
- - -    - - -         - - -      - - -
我方      对手          我方       对手
```
在上图中如果只看我方走棋完成后的棋局状态，它们状态是不同的，因此会存在两个状态价值，而采用后位状态后，最终看的是对手下完子时的棋局状态，这样最终形成的棋局状态是同一个，状态价值只有一个。（可以看作是未知环境对于转移后状态再作用一次，这个在前面有风的格子世界问题中已经碰到了）
